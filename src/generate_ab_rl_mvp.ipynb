{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:54.057110Z",
     "start_time": "2019-02-26T11:09:54.034888Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# z test\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# bayesian bootstrap and vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bayesian_bootstrap.bootstrap as bb\n",
    "from astropy.utils import NumpyRNGContext\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# set up the style for our plots\n",
    "sns.set(style='white', palette='colorblind', font_scale=1.3,\n",
    "        rc={'figure.figsize':(12,9), \n",
    "            \"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# instantiate progress bar goodness\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "# the number of bootstrap means used to generate a distribution\n",
    "boot_reps = 10000\n",
    "\n",
    "# alpha - false positive rate\n",
    "alpha = 0.05\n",
    "# number of tests\n",
    "m = 4\n",
    "# Correct alpha for multiple comparisons\n",
    "alpha = alpha / m\n",
    "\n",
    "# The Bonferroni correction can be used to adjust confidence intervals also. \n",
    "# If one establishes m confidence intervals, and wishes to have an overall confidence level of 1-alpha,\n",
    "# each individual confidence interval can be adjusted to the level of 1-(alpha/m).\n",
    "\n",
    "# reproducible\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File/dir locations\n",
    "### Processed journey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:54.065742Z",
     "start_time": "2019-02-26T11:09:54.060585Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "filename = \"testing_processed_sampled_taxon_ab_2019-01-21.csv.gz\"\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"sampled_journey\",\n",
    "    filename)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:55.445514Z",
     "start_time": "2019-02-26T11:09:54.068201Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in processed sampled journey with just the cols we need for related links\n",
    "df = pd.read_csv(filepath, sep =\"\\t\", compression=\"gzip\")\n",
    "# convert from str to list\n",
    "df['Event_cat_act_agg']= df['Event_cat_act_agg'].progress_apply(ast.literal_eval)\n",
    "df['Page_Event_List'] = df['Page_Event_List'].progress_apply(ast.literal_eval)\n",
    "df['Page_List'] = df['Page_List'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:55.470863Z",
     "start_time": "2019-02-26T11:09:55.448986Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Page_List_Length'] = df['Page_List'].progress_apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:55.487014Z",
     "start_time": "2019-02-26T11:09:55.474248Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop dodgy rows, where page variant is not A or B. \n",
    "df = df.query('ABVariant in [\"A\", \"B\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nav type of page lookup - is it a finding page? if not it's a thing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.140448Z",
     "start_time": "2019-02-26T11:09:55.489204Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"document_types.csv.gz\"\n",
    "\n",
    "# created a metadata dir in the DATA_DIR to hold this data\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"metadata\",\n",
    "    filename)\n",
    "print(filepath)\n",
    "\n",
    "df_finding_thing = pd.read_csv(filepath, sep=\"\\t\", compression=\"gzip\")\n",
    "\n",
    "df_finding_thing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.358643Z",
     "start_time": "2019-02-26T11:09:59.142775Z"
    }
   },
   "outputs": [],
   "source": [
    "thing_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==0]['pagePath'].tolist()\n",
    "\n",
    "\n",
    "finding_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==1]['pagePath'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Some rows should be removed before analysis. For example rows with journey lengths of 500 or very high related link click rates. This process might have to happen once features have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## journey_click_rate\n",
    "There is no difference in the proportion of journeys using at least one related link (journey_click_rate) between page variant A and page variant B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys including at least one click on a related link}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related link prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.425666Z",
     "start_time": "2019-02-26T11:09:59.360786Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_events_rl(event):\n",
    "    \"\"\"Counts events with category 'relatedLinkClicked' and action'Related content'.\"\"\"\n",
    "    if event[0][0] == 'relatedLinkClicked' and 'Related content' in event[0][1]:\n",
    "        return event[1]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sum_related_click_events(event_list):\n",
    "    return sum([get_number_of_events_rl(event) for event in event_list])\n",
    "\n",
    "\n",
    "def is_related(x):\n",
    "    \"\"\"Compute whether a journey includes at least one related link click.\"\"\"\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.447002Z",
     "start_time": "2019-02-26T11:09:59.428798Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of related links clicks per Sequence\n",
    "df['Related Links Clicks per seq'] = df['Event_cat_act_agg'].map(sum_related_click_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.459475Z",
     "start_time": "2019-02-26T11:09:59.449460Z"
    }
   },
   "outputs": [],
   "source": [
    "# map across the Sequence variable, which includes pages and Events\n",
    "# we want to pass all the list elements to a function one-by-one and then collect the output.\n",
    "df[\"Has_Related\"] = df[\"Related Links Clicks per seq\"].map(is_related)\n",
    "\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T11:09:59.491111Z",
     "start_time": "2019-02-26T11:09:59.461667Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.455941Z",
     "start_time": "2019-02-25T22:27:43.363422Z"
    }
   },
   "outputs": [],
   "source": [
    "def z_prop(df, col_name):\n",
    "    \"\"\"\n",
    "    Conduct z_prop test and generate confidence interval.\n",
    "\n",
    "    Using Bernoulli trial terminology where X (or x)\n",
    "    is number of successes and n is number of trials\n",
    "    total occurrences, we compare ABVariant A and B.\n",
    "    p is x/n. We use a z proportion test between variants.\n",
    "    \"\"\"\n",
    "    # A & B\n",
    "    n = df.Occurrences.sum()\n",
    "    # prop of journeys with at least one related link, occurrences summed for those rows gives X\n",
    "    p = df[df[col_name] == 1].Occurrences.sum() / n\n",
    "\n",
    "    assert (p >= 0), \"Prop less than zero!\"\n",
    "    assert (p <= 1), \"Prop greater than one!\"\n",
    "\n",
    "    # A\n",
    "    # number of trials for page A\n",
    "    n_a = df[df.ABVariant == \"A\"].Occurrences.sum()\n",
    "    # number of successes (occurrences), for page A and at least one related link clicked journeys\n",
    "    x_a = df[(df['ABVariant'] == 'A') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on A\n",
    "    p_a = x_a / n_a\n",
    "\n",
    "    # B\n",
    "    # number of trials for page B\n",
    "    n_b = df[df.ABVariant == \"B\"].Occurrences.sum()\n",
    "    # number of successes for page B, at least one related link clicked\n",
    "    x_b = df[(df['ABVariant'] == 'B') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on B\n",
    "    p_b = x_b / n_b\n",
    "\n",
    "    assert (n == n_a + n_b), \"Error in filtering by ABVariant!\"\n",
    "\n",
    "    # validate assumptions\n",
    "    # The formula of z-statistic is valid only when sample size (n) is large enough.\n",
    "    # nAp, nAq, nBp and nBq should be ≥ 5.\n",
    "    # where p is probability of success (we can use current baseline)\n",
    "    # q = 1 - p\n",
    "\n",
    "    # tried a helper function here but it didn't work hence not DRY\n",
    "    assert (n_a * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_a * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    assert (n_b * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_b * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    # using statsmodels\n",
    "    # successes\n",
    "    count = np.array([x_a, x_b])\n",
    "    # number of trials\n",
    "    nobs = np.array([n_a, n_b])\n",
    "    # z prop test\n",
    "    z, p_value = proportions_ztest(count, nobs, value=0, alternative='two-sided')\n",
    "    # print(' z-stat = {z} \\n p-value = {p_value}'.format(z=z,p_value=p_value))\n",
    "\n",
    "    statsdict = {'metric_name': col_name, 'stats_method': 'z_prop_test',\n",
    "                 'x_ab': x_a + x_b, 'n_ab': n, 'p': p,\n",
    "                 'x_a': x_a, 'n_a': n_a, 'p_a': p_a,\n",
    "                 'x_b': x_b, 'n_b': n_b, 'p_b': p_b,\n",
    "                 'test_statistic': z, 'p-value': p_value}\n",
    "\n",
    "    return statsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.460606Z",
     "start_time": "2019-02-25T22:27:43.458299Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(proportions_ztest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.488993Z",
     "start_time": "2019-02-25T22:27:43.462628Z"
    }
   },
   "outputs": [],
   "source": [
    "has_rel = z_prop(df, 'Has_Related')\n",
    "has_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.501378Z",
     "start_time": "2019-02-25T22:27:43.491750Z"
    }
   },
   "outputs": [],
   "source": [
    "# uplift\n",
    "def compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b):\n",
    "    \"\"\"\n",
    "    The standard error of the difference between two proportions is given by the square root of the variances.\n",
    "    \n",
    "    The square of the standard error of a proportion is known as the variance of proportion. \n",
    "    The variance of the difference between two independent proportions is equal to the sum of the variances of the proportions of each sample. \n",
    "    The variances are summed because each sample contributes to sampling error in the distribution of differences.\n",
    "    \n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = p1*(1-p1)/n_a + p2*(1-p2)/n_b\n",
    "    return np.sqrt(se)\n",
    "    \n",
    "def zconf_interval_two_samples(x_a, n_a, x_b, n_b, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Gives two points, the lower and upper bound of a (1-alpha)% confidence interval.\n",
    "    \n",
    "    To calculate the confidence interval we need to know the standard error of the difference between two proportions. \n",
    "    The standard error of the difference between two proportions is the combination of the standard error of two independent distributions, ES (p_a) and (p_b).\n",
    "    \n",
    "    If the CI includes zero then we accept the null hypothesis at the defined alpha.\n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b)\n",
    "    z_critical = stats.norm.ppf(1-0.5*alpha)\n",
    "    return p2-p1-z_critical*se, p2-p1+z_critical*se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.508821Z",
     "start_time": "2019-02-25T22:27:43.503914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(has_rel['x_a'], has_rel['n_a'],\n",
    "                                           has_rel['x_b'], has_rel['n_b'], alpha = 0.01)\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.514716Z",
     "start_time": "2019-02-25T22:27:43.511943Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@thibalbo/coding-bayesian-ab-tests-in-python-e89356b3f4bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be developed, a Bayesian approach can provide a simpler interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count of clicks on navigation elements\n",
    "\n",
    "There is no statistically significant difference in the count of clicks on navigation elements per journey between page variant A and page variant B.\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\text{total number of navigation element click events from content pages}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:12:55.711730Z",
     "start_time": "2019-02-14T22:12:55.709272Z"
    }
   },
   "source": [
    "### Related link counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.530095Z",
     "start_time": "2019-02-25T22:27:43.525365Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the total number of related links clicks for that row (clicks per sequence multiplied by occurrences)\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:43.542933Z",
     "start_time": "2019-02-25T22:27:43.535491Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_nav_event(event):\n",
    "    \"\"\"\n",
    "    Return the total number of related links clicks for that row.\n",
    "    \n",
    "    Clicks per sequence multiplied by occurrences. \n",
    "    \"\"\"\n",
    "    return any(\n",
    "        ['breadcrumbClicked' in event, 'homeLinkClicked' in event,\n",
    "         all(cond in event for cond in [\n",
    "             'relatedLinkClicked','Explore the topic'])])\n",
    "\n",
    "\n",
    "def count_nav_events(page_event_list):\n",
    "    \"\"\"Counts the number of nav events from a content page in a Page Event List.\"\"\"\n",
    "    content_page_nav_events = 0\n",
    "    for pair in page_event_list:\n",
    "        if is_nav_event(pair[1]):\n",
    "            if pair[0] in thing_page_paths:\n",
    "                content_page_nav_events += 1\n",
    "    return content_page_nav_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:50.104542Z",
     "start_time": "2019-02-25T22:27:43.545179Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# needs finding_thing_df read in from document_types.csv.gz\n",
    "df['Content_Page_Nav_Event_Count'] = df['Page_Event_List'].progress_map(count_nav_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:27:50.111624Z",
     "start_time": "2019-02-25T22:27:50.106729Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_search_from_content(page_list):\n",
    "    search_from_content = 0\n",
    "    for i, page in enumerate(page_list):\n",
    "        if i > 0:\n",
    "            if '/search?q=' in page:\n",
    "                if page_list[i-1] in thing_page_paths:\n",
    "                    search_from_content += 1\n",
    "    return search_from_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.660148Z",
     "start_time": "2019-02-25T22:27:50.114721Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Content_Search_Event_Count'] = df['Page_List'].progress_map(count_search_from_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the derived metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.672955Z",
     "start_time": "2019-02-25T22:28:01.662593Z"
    }
   },
   "outputs": [],
   "source": [
    "# count of nav or search clicks\n",
    "df['Content_Nav_or_Search_Count'] = df['Content_Page_Nav_Event_Count'] + df['Content_Search_Event_Count']\n",
    "# occurrences is accounted for by the group by bit in our bayesian boot analysis function\n",
    "df['Content_Nav_Search_Event_Sum_row_total'] = df['Content_Nav_or_Search_Count'] * df['Occurrences']\n",
    "# required for journeys with no nav later\n",
    "df['Has_No_Nav_Or_Search'] = df['Content_Nav_Search_Event_Sum_row_total'] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vestigial metric which we dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.679038Z",
     "start_time": "2019-02-25T22:28:01.675511Z"
    }
   },
   "outputs": [],
   "source": [
    "# (nav events + search events) + 1 / related links clicked + 1\n",
    "# add one to numerator and denominator to avoid undesirable characteristics\n",
    "# not sure this has great utility as a proxy, seems volatile\n",
    "# df['Ratio_Nav_Search_to_Rel'] = (df['Content_Nav_Search_Event_Sum_row_total'] + 1) / (df['Related Links Clicks row total'] + 1)\n",
    "# sns.distplot(df['Ratio_Nav_Search_to_Rel'].values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This derived variable is problematic, should consider dropping it. Use counts of the numerator instead (as this could be modelled using generalised linear model), as related link clickedness is captured by the earlier metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary df file in case of crash\n",
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.686238Z",
     "start_time": "2019-02-25T22:28:01.682375Z"
    }
   },
   "outputs": [],
   "source": [
    "# create temp file incase bootstrap below crashes\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"rl_sampled_processed_journey\",\n",
    "    filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.691295Z",
     "start_time": "2019-02-25T22:28:01.688454Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv(filepath, sep=\"\\t\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the above as the Bayesian boostrap is computationally intensive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.697640Z",
     "start_time": "2019-02-25T22:28:01.693323Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_bb(counter_X_keys, counter_X_vals, n_replications):\n",
    "    \"\"\"Simulate the posterior distribution of the mean.\n",
    "    Parameter X: The observed data (array like)\n",
    "    Parameter n_replications: The number of bootstrap replications to perform (positive integer)\n",
    "    Returns: Samples from the posterior\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    weights = np.random.dirichlet(counter_X_vals, n_replications)\n",
    "    for w in weights:\n",
    "        samples.append(np.dot(counter_X_keys, w))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:01.706442Z",
     "start_time": "2019-02-25T22:28:01.699512Z"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bootstrap_analysis(df, col_name=None, boot_reps = 10000, seed = 1337):\n",
    "    \"\"\"Run bayesian bootstrap on the mean of a variable of interest between Page Variants.\n",
    "    \n",
    "    Args:\n",
    "        df: A rl_sampled_processed pandas Datframe.\n",
    "        col_name: A string of the column of interest.\n",
    "\n",
    "    Returns:\n",
    "        a_bootstrap: a vector of boot_reps n resampled means from A.\n",
    "        b_bootstrap: a vector of boot_reps n resampled means from B.\n",
    "        \"\"\"\n",
    "    # grouped by length is vestigial, before this was generalised into a function\n",
    "    with NumpyRNGContext(seed):\n",
    "        A_grouped_by_length =  df[df.ABVariant == \"A\"].groupby(\n",
    "            col_name).sum().reset_index()\n",
    "        B_grouped_by_length =  df[df.ABVariant == \"B\"].groupby(\n",
    "            col_name).sum().reset_index()\n",
    "        a_bootstrap = mean_bb(A_grouped_by_length[col_name], \n",
    "        A_grouped_by_length['Occurrences'], \n",
    "                                 boot_reps)\n",
    "        b_bootstrap = mean_bb(B_grouped_by_length[col_name], \n",
    "                                 B_grouped_by_length['Occurrences'], \n",
    "                                 boot_reps)\n",
    "    \n",
    "    return a_bootstrap, b_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:02.907400Z",
     "start_time": "2019-02-25T22:28:01.708547Z"
    }
   },
   "outputs": [],
   "source": [
    "a_bootstrap, b_bootstrap = bayesian_bootstrap_analysis(df, col_name='Content_Nav_or_Search_Count', boot_reps=boot_reps, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:02.923839Z",
     "start_time": "2019-02-25T22:28:02.909891Z"
    }
   },
   "outputs": [],
   "source": [
    "def bb_hdi(a_bootstrap, b_bootstrap, alpha = 0.05):\n",
    "    \"\"\"Calculate a 1-alpha high density interval\n",
    "    \n",
    "    Args:\n",
    "        a_bootstrap: a list of resampled means from page A journeys.\n",
    "        b_bootstrap: a list of resampled means from page B journeys.\n",
    "\n",
    "    Returns:\n",
    "        a_ci_low: the lower point of the 1-alpha% highest density interval for A.\n",
    "        a_ci_hi: the higher point of the 1-alpha% highest density interval for A.\n",
    "        b_ci_low: the lower point of the 1-alpha% highest density interval for B.\n",
    "        b_ci_hi: the higher point of the 1-alpha% highest density interval for B.\n",
    "        ypa_diff_mean: the mean difference for the posterior between A's and B's distributions.\n",
    "        ypa_diff_ci_low: lower hdi for posterior of the difference.\n",
    "        ypa_diff_ci_hi: upper hdi for posterior of the difference.\n",
    "        sorta_p_value: number of values greater than 0 divided by num of obs for mean diff psoterior.\n",
    "        \"\"\"\n",
    "    # Calculate a 95% HDI\n",
    "    a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap, alpha=alpha)\n",
    "    # Calculate a 95% HDI\n",
    "    b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap, alpha=alpha)\n",
    "    \n",
    "    # calculate the posterior for the difference between A's and B's mean of resampled means\n",
    "    # ypa prefix is vestigial from blog post\n",
    "    ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "    ypa_diff_mean = ypa_diff.mean()\n",
    "    # get the hdi\n",
    "    ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "    # We count the number of values greater than 0 and divide by the total number\n",
    "    # of observations\n",
    "    # which returns us the the proportion of values in the distribution that are\n",
    "    # greater than 0, could act a bit like a p-value\n",
    "    p_value = (ypa_diff > 0).sum() / ypa_diff.shape[0]\n",
    "\n",
    "    return {'a_ci_low':a_ci_low, 'a_ci_hi':a_ci_hi, 'b_ci_low':b_ci_low, 'b_ci_hi':b_ci_hi, 'ypa_diff_mean':ypa_diff_mean, 'ypa_diff_ci_low':ypa_diff_ci_low, 'ypa_diff_ci_hi':ypa_diff_ci_hi, 'p_value':p_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:02.949780Z",
     "start_time": "2019-02-25T22:28:02.926537Z"
    }
   },
   "outputs": [],
   "source": [
    "# ratio is vestigial but we keep it here for convenience\n",
    "# it's actually a count but considers occurrences\n",
    "ratio_stats = bb_hdi(a_bootstrap, b_bootstrap)\n",
    "ratio_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:02.957403Z",
     "start_time": "2019-02-25T22:28:02.952124Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:03.644578Z",
     "start_time": "2019-02-25T22:28:02.959463Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax.errorbar(x=[ratio_stats['b_ci_low'], ratio_stats['b_ci_hi']], y=[2, 2], linewidth=5, c='teal', marker='o', \n",
    "         label='95% HDI B')\n",
    "\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.errorbar(x=[ratio_stats['a_ci_low'], ratio_stats['a_ci_hi']], y=[5, 5], linewidth=5, c='salmon', marker='o', \n",
    "         label='95% HDI A')\n",
    "\n",
    "ax.set(xlabel='Content_Nav_or_Search_Count', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:03.661789Z",
     "start_time": "2019-02-25T22:28:03.647174Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's ratio\n",
    "# ypa prefix is vestigial from blog post\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "\n",
    "# the mean of the posterior\n",
    "print('mean:', ypa_diff.mean())\n",
    "\n",
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.054781Z",
     "start_time": "2019-02-25T22:28:03.665092Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Content_Nav_or_Search_Count', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean counts times occurrences')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.063483Z",
     "start_time": "2019-02-25T22:28:04.057528Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proportion of journeys with a page sequence including content and related links only\n",
    "\n",
    "There is no statistically significant difference in the proportion of journeys with a page sequence including content and related links only (including loops) between page variant A and page variant B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys that only contain content pages and related links (i.e. no nav pages)}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.075096Z",
     "start_time": "2019-02-25T22:28:04.066720Z"
    }
   },
   "outputs": [],
   "source": [
    "# if (Content_Nav_Search_Event_Sum == 0) that's our success\n",
    "# Has_No_Nav_Or_Search == 1 is a success\n",
    "# the problem is symmetrical so doesn't matter too much\n",
    "sum(df.Has_No_Nav_Or_Search * df.Occurrences) / df.Occurrences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.363684Z",
     "start_time": "2019-02-25T22:28:04.077537Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.Content_Nav_or_Search_Count.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T17:13:51.806312Z",
     "start_time": "2019-02-13T17:13:51.803513Z"
    }
   },
   "source": [
    "### Frequentist statistics\n",
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.388828Z",
     "start_time": "2019-02-25T22:28:04.366157Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nav = z_prop(df, 'Has_No_Nav_Or_Search')\n",
    "nav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.397875Z",
     "start_time": "2019-02-25T22:28:04.391626Z"
    }
   },
   "outputs": [],
   "source": [
    "# function defined earlier in notebook\n",
    "\n",
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(nav['x_a'], nav['n_a'],\n",
    "                                           nav['x_b'], nav['n_b'], alpha = 0.01)\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Journey Length (number of page views)\n",
    "There is no statistically significant difference in the average page list length of journeys (including loops) between page variant A and page variant B.\n",
    "### Bayesian bootstrap for non-parametric hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.404516Z",
     "start_time": "2019-02-25T22:28:04.401235Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://savvastjortjoglou.com/nfl-bayesian-bootstrap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.409898Z",
     "start_time": "2019-02-25T22:28:04.407038Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's use mean journey length (could probably model parametrically but we use it for demonstration here)\n",
    "# some journeys have length 500 and should probably be removed as they are liekely bots or other weirdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.425142Z",
     "start_time": "2019-02-25T22:28:04.412539Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to roll out the data, deaggregate on one variable of interest\n",
    "# we want to repeat each row's journey length by it's occurrences\n",
    "# so more common journey lengths are more likely to be sampled\n",
    "print(df['Page_List_Length'].head())\n",
    "print(df['Occurrences'].head())\n",
    "\n",
    "np.repeat(df['Page_List_Length'].head(), df['Occurrences'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.444104Z",
     "start_time": "2019-02-25T22:28:04.428469Z"
    }
   },
   "outputs": [],
   "source": [
    "a_len = np.repeat(df.loc[df.ABVariant == \"A\", 'Page_List_Length'], df.loc[df.ABVariant == \"A\", \"Occurrences\"])\n",
    "a_len.values\n",
    "\n",
    "b_len = np.repeat(df.loc[df.ABVariant == \"B\", 'Page_List_Length'], df.loc[df.ABVariant == \"B\", \"Occurrences\"])\n",
    "b_len.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:04.450644Z",
     "start_time": "2019-02-25T22:28:04.446887Z"
    }
   },
   "outputs": [],
   "source": [
    "help(bb.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:08.371634Z",
     "start_time": "2019-02-25T22:28:04.453791Z"
    }
   },
   "outputs": [],
   "source": [
    "# for reproducibility, set the seed within this context\n",
    "with NumpyRNGContext(1337):\n",
    "    a_bootstrap = bb.mean(a_len.values, n_replications=boot_reps)\n",
    "    b_bootstrap = bb.mean(b_len.values, n_replications=boot_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:08.722603Z",
     "start_time": "2019-02-25T22:28:08.374075Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(a_bootstrap, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant A Mean Journey Length')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:08.735183Z",
     "start_time": "2019-02-25T22:28:08.725421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap)\n",
    "print('low ci:', a_ci_low, '\\nhigh ci:', a_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:09.124100Z",
     "start_time": "2019-02-25T22:28:08.738761Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(a_bootstrap, color='salmon')\n",
    "ax.plot([a_ci_low, a_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant A Mean Journey Length')\n",
    "sns.despine()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:09.135879Z",
     "start_time": "2019-02-25T22:28:09.126810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap)\n",
    "print('low ci:', b_ci_low, '\\nhigh ci:', b_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:09.498738Z",
     "start_time": "2019-02-25T22:28:09.139261Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap)\n",
    "ax.plot([b_ci_low, b_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant B Mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:09.996110Z",
     "start_time": "2019-02-25T22:28:09.501731Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the uncertainty in the difference between the Page Variants's Journey Length by subtracting their posteriors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:10.012799Z",
     "start_time": "2019-02-25T22:28:09.998783Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's YPA\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:10.024348Z",
     "start_time": "2019-02-25T22:28:10.016247Z"
    }
   },
   "outputs": [],
   "source": [
    "# the mean of the posterior\n",
    "ypa_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:10.031669Z",
     "start_time": "2019-02-25T22:28:10.026603Z"
    }
   },
   "outputs": [],
   "source": [
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:10.433167Z",
     "start_time": "2019-02-25T22:28:10.034440Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually calculate the probability that B's mean Journey Length was greater than A's mean Journey Length by measuring the proportion of values greater than 0 in the above distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:28:10.444271Z",
     "start_time": "2019-02-25T22:28:10.437599Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.15px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
