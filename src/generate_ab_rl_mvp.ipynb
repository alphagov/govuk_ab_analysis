{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:29.629511Z",
     "start_time": "2019-02-20T21:40:27.939494Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# z test\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# bayesian bootstrap and vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bayesian_bootstrap.bootstrap as bb\n",
    "from astropy.utils import NumpyRNGContext\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# set up the style for our plots\n",
    "sns.set(style='white', palette='colorblind', font_scale=1.3,\n",
    "        rc={'figure.figsize':(12,9), \n",
    "            \"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# instantiate progress bar goodness\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "# if you have more compute set to 10,000 ideally\n",
    "# the number of bootstrap means used to generate a distribution\n",
    "boot_reps = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File/dir locations\n",
    "### Processed journey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:29.640139Z",
     "start_time": "2019-02-20T21:40:29.632813Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "filename = \"testing_processed_sampled_taxon_ab_2019-01-21.csv.gz\"\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"processed_journey\",\n",
    "    filename)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:30.923695Z",
     "start_time": "2019-02-20T21:40:29.642321Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in processed sampled journey with just the cols we need for related links\n",
    "df = pd.read_csv(filepath, sep =\"\\t\", compression=\"gzip\")\n",
    "# convert from str to list\n",
    "df['Event_cat_act_agg']= df['Event_cat_act_agg'].progress_apply(ast.literal_eval)\n",
    "df['Page_Event_List'] = df['Page_Event_List'].progress_apply(ast.literal_eval)\n",
    "df['Page_List'] = df['Page_List'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:30.946162Z",
     "start_time": "2019-02-20T21:40:30.927031Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Page_List_Length'] = df['Page_List'].progress_apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:30.961191Z",
     "start_time": "2019-02-20T21:40:30.949888Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop dodgy rows, where page variant is not A or B. \n",
    "df = df.query('ABVariant in [\"A\", \"B\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nav type of page lookup - is it a finding page? if not it's a thing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.614231Z",
     "start_time": "2019-02-20T21:40:30.962817Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"document_types.csv.gz\"\n",
    "\n",
    "# created a metadata dir in the DATA_DIR to hold this data\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"metadata\",\n",
    "    filename)\n",
    "print(filepath)\n",
    "\n",
    "df_finding_thing = pd.read_csv(filepath, sep=\"\\t\", compression=\"gzip\")\n",
    "\n",
    "df_finding_thing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.826810Z",
     "start_time": "2019-02-20T21:40:34.616373Z"
    }
   },
   "outputs": [],
   "source": [
    "thing_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==0]['pagePath'].tolist()\n",
    "\n",
    "\n",
    "finding_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==1]['pagePath'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Some rows should be removed before analysis. For example rows with journey lengths of 500 or very high related link click rates. This process might have to happen once features have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## journey_click_rate\n",
    "There is no difference in the proportion of journeys using at least one related link (journey_click_rate) between page variant A and page variant B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys including at least one click on a related link}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related link prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.835472Z",
     "start_time": "2019-02-20T21:40:34.829153Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_events_rl(event):\n",
    "    \"\"\"Counts events with category 'relatedLinkClicked' and action'Related content'.\"\"\"\n",
    "    if event[0][0] == 'relatedLinkClicked' and 'Related content' in event[0][1]:\n",
    "        return event[1]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sum_related_click_events(event_list):\n",
    "    return sum([get_number_of_events_rl(event) for event in event_list])\n",
    "\n",
    "\n",
    "def is_related(x):\n",
    "    \"\"\"Compute whether a journey includes at least one related link click.\"\"\"\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.855252Z",
     "start_time": "2019-02-20T21:40:34.838126Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of related links clicks per Sequence\n",
    "df['Related Links Clicks per seq'] = df['Event_cat_act_agg'].map(sum_related_click_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.865362Z",
     "start_time": "2019-02-20T21:40:34.857461Z"
    }
   },
   "outputs": [],
   "source": [
    "# map across the Sequence variable, which includes pages and Events\n",
    "# we want to pass all the list elements to a function one-by-one and then collect the output.\n",
    "df[\"Has_Related\"] = df[\"Related Links Clicks per seq\"].map(is_related)\n",
    "\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.896891Z",
     "start_time": "2019-02-20T21:40:34.867332Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.987711Z",
     "start_time": "2019-02-20T21:40:34.898788Z"
    }
   },
   "outputs": [],
   "source": [
    "def z_prop(df, col_name):\n",
    "    \"\"\"\n",
    "    Conduct z_prop test and generate confidence interval.\n",
    "\n",
    "    Using Bernoulli trial terminology where X (or x)\n",
    "    is number of successes and n is number of trials\n",
    "    total occurrences, we compare ABVariant A and B.\n",
    "    p is x/n. We use a z proportion test between variants.\n",
    "    \"\"\"\n",
    "    # A & B\n",
    "    n = df.Occurrences.sum()\n",
    "    # prop of journeys with at least one related link, occurrences summed for those rows gives X\n",
    "    p = df[df[col_name] == 1].Occurrences.sum() / n\n",
    "\n",
    "    assert (p >= 0), \"Prop less than zero!\"\n",
    "    assert (p <= 1), \"Prop greater than one!\"\n",
    "\n",
    "    # A\n",
    "    # number of trials for page A\n",
    "    n_a = df[df.ABVariant == \"A\"].Occurrences.sum()\n",
    "    # number of successes (occurrences), for page A and at least one related link clicked journeys\n",
    "    x_a = df[(df['ABVariant'] == 'A') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on A\n",
    "    p_a = x_a / n_a\n",
    "\n",
    "    # B\n",
    "    # number of trials for page B\n",
    "    n_b = df[df.ABVariant == \"B\"].Occurrences.sum()\n",
    "    # number of successes for page B, at least one related link clicked\n",
    "    x_b = df[(df['ABVariant'] == 'B') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on B\n",
    "    p_b = x_b / n_b\n",
    "\n",
    "    assert (n == n_a + n_b), \"Error in filtering by ABVariant!\"\n",
    "\n",
    "    # validate assumptions\n",
    "    # The formula of z-statistic is valid only when sample size (n) is large enough.\n",
    "    # nAp, nAq, nBp and nBq should be ≥ 5.\n",
    "    # where p is probability of success (we can use current baseline)\n",
    "    # q = 1 - p\n",
    "\n",
    "    # tried a helper function here but it didn't work hence not DRY\n",
    "    assert (n_a * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_a * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    assert (n_b * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_b * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    # using statsmodels\n",
    "    # successes\n",
    "    count = np.array([x_a, x_b])\n",
    "    # number of trials\n",
    "    nobs = np.array([n_a, n_b])\n",
    "    # z prop test\n",
    "    z, p_value = proportions_ztest(count, nobs, value=0, alternative='two-sided')\n",
    "    # print(' z-stat = {z} \\n p-value = {p_value}'.format(z=z,p_value=p_value))\n",
    "\n",
    "    statsdict = {'metric_name': col_name, 'stats_method': 'z_prop_test',\n",
    "                 'x_ab': x_a + x_b, 'n_ab': n, 'p': p,\n",
    "                 'x_a': x_a, 'n_a': n_a, 'p_a': p_a,\n",
    "                 'x_b': x_b, 'n_b': n_b, 'p_b': p_b,\n",
    "                 'test_statistic': z, 'p-value': p_value}\n",
    "\n",
    "    return statsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:34.993690Z",
     "start_time": "2019-02-20T21:40:34.990296Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(proportions_ztest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.016677Z",
     "start_time": "2019-02-20T21:40:34.995685Z"
    }
   },
   "outputs": [],
   "source": [
    "has_rel = z_prop(df, 'Has_Related')\n",
    "has_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.026891Z",
     "start_time": "2019-02-20T21:40:35.018600Z"
    }
   },
   "outputs": [],
   "source": [
    "# uplift\n",
    "def compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b):\n",
    "    \"\"\"\n",
    "    The standard error of the difference between two proportions is given by the square root of the variances.\n",
    "    \n",
    "    The square of the standard error of a proportion is known as the variance of proportion. \n",
    "    The variance of the difference between two independent proportions is equal to the sum of the variances of the proportions of each sample. \n",
    "    The variances are summed because each sample contributes to sampling error in the distribution of differences.\n",
    "    \n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = p1*(1-p1)/n_a + p2*(1-p2)/n_b\n",
    "    return np.sqrt(se)\n",
    "    \n",
    "def zconf_interval_two_samples(x_a, n_a, x_b, n_b, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Gives two points, the lower and upper bound of a (1-alpha)% confidence interval.\n",
    "    \n",
    "    To calculate the confidence interval we need to know the standard error of the difference between two proportions. \n",
    "    The standard error of the difference between two proportions is the combination of the standard error of two independent distributions, ES (p_a) and (p_b).\n",
    "    \n",
    "    If the CI includes zero then we accept the null hypothesis at the defined alpha.\n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b)\n",
    "    z_critical = stats.norm.ppf(1-0.5*alpha)\n",
    "    return p2-p1-z_critical*se, p2-p1+z_critical*se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.034257Z",
     "start_time": "2019-02-20T21:40:35.029271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(has_rel['x_a'], has_rel['n_a'],\n",
    "                                           has_rel['x_b'], has_rel['n_b'], alpha = 0.01)\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.039658Z",
     "start_time": "2019-02-20T21:40:35.036841Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@thibalbo/coding-bayesian-ab-tests-in-python-e89356b3f4bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be developed, a Bayesian approach can provide a simpler interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ratio of clicks on navigation elements vs. clicks on related links\n",
    "\n",
    "There is no statistically significant difference in the ratio of clicks on navigation elements vs. clicks on related links between page variant A and page variant B\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of navigation element click events from content pages}}{\\text{total number of related link click events}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:12:55.711730Z",
     "start_time": "2019-02-14T22:12:55.709272Z"
    }
   },
   "source": [
    "### Related link counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.048855Z",
     "start_time": "2019-02-20T21:40:35.044663Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the total number of related links clicks for that row (clicks per sequence multiplied by occurrences)\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:35.059974Z",
     "start_time": "2019-02-20T21:40:35.052269Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_nav_event(event):\n",
    "    \"\"\"\n",
    "    Return the total number of related links clicks for that row.\n",
    "    \n",
    "    Clicks per sequence multiplied by occurrences. \n",
    "    \"\"\"\n",
    "    return any(\n",
    "        ['breadcrumbClicked' in event, 'homeLinkClicked' in event,\n",
    "         all(cond in event for cond in [\n",
    "             'relatedLinkClicked','Explore the topic'])])\n",
    "\n",
    "\n",
    "def count_nav_events(page_event_list):\n",
    "    \"\"\"Counts the number of nav events from a content page in a Page Event List.\"\"\"\n",
    "    content_page_nav_events = 0\n",
    "    for pair in page_event_list:\n",
    "        if is_nav_event(pair[1]):\n",
    "            if pair[0] in thing_page_paths:\n",
    "                content_page_nav_events += 1\n",
    "    return content_page_nav_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:41.154451Z",
     "start_time": "2019-02-20T21:40:35.062120Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# needs finding_thing_df read in from document_types.csv.gz\n",
    "df['Content_Page_Nav_Event_Count'] = df['Page_Event_List'].progress_map(count_nav_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:41.160992Z",
     "start_time": "2019-02-20T21:40:41.156470Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_search_from_content(page_list):\n",
    "    search_from_content = 0\n",
    "    for i, page in enumerate(page_list):\n",
    "        if i > 0:\n",
    "            if '/search?q=' in page:\n",
    "                if page_list[i-1] in thing_page_paths:\n",
    "                    search_from_content += 1\n",
    "    return search_from_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:52.509611Z",
     "start_time": "2019-02-20T21:40:41.164165Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Content_Search_Event_Count'] = df['Page_List'].progress_map(count_search_from_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the derived metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:52.958059Z",
     "start_time": "2019-02-20T21:40:52.511928Z"
    }
   },
   "outputs": [],
   "source": [
    "# potential BUG? is this correctly derived? confirm with colleague\n",
    "# (nav events + search events) + 1 / related links clicked + 1\n",
    "# add one to numerator and denominator to avoid undesirable characteristics\n",
    "# not sure this has great utility as a proxy, seems volatile\n",
    "df['Ratio_Nav_Search_to_Rel'] = (df['Content_Page_Nav_Event_Count'] + df.Content_Search_Event_Count + 1) / (df['Related Links Clicks row total'] + 1)\n",
    "sns.distplot(df['Ratio_Nav_Search_to_Rel'].values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This derived variable is problematic, should consider dropping it. Use counts of the numerator instead (as this could be modelled using generalised linear model), as related link clickedness is captured by the earlier metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempoary df file in case of crash\n",
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:52.964881Z",
     "start_time": "2019-02-20T21:40:52.960754Z"
    }
   },
   "outputs": [],
   "source": [
    "# create temp file incase bootstrap below crashes\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"rl_sampled_processed_journey\",\n",
    "    filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:52.970724Z",
     "start_time": "2019-02-20T21:40:52.968018Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv(filepath, sep=\"\\t\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the above as the Bayesian boostrap is computationally intensive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:52.981694Z",
     "start_time": "2019-02-20T21:40:52.973318Z"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bootstrap_analysis(df, col_name=None, boot_reps=4000, seed = 1337):\n",
    "    \"\"\"Run bayesian bootstrap on the mean of a variable of interest between Page Variants.\n",
    "    \n",
    "    Args:\n",
    "        df: A rl_sampled_processed pandas Datframe.\n",
    "        col_name: A string of the column of interest.\n",
    "\n",
    "    Returns:\n",
    "        a_bootstrap: a vector of boot_reps n resampled means from A.\n",
    "        b_bootstrap: a vector of boot_reps n resampled means from B.\n",
    "        \"\"\"\n",
    "    # need to roll out the data, to resample from, deaggregate on one variable of interest\n",
    "    # we want to repeat each row's journey length by it's occurrences\n",
    "    # so more common journey lengths are more likely to be sampled\n",
    "    a_r = np.repeat(df.loc[df.ABVariant == \"A\", col_name], df.loc[df.ABVariant == \"A\", \"Occurrences\"])\n",
    "    b_r = np.repeat(df.loc[df.ABVariant == \"B\", col_name], df.loc[df.ABVariant == \"B\", \"Occurrences\"])\n",
    "    # for reproducibility, set the seed within this context\n",
    "    with NumpyRNGContext(seed):\n",
    "        a_bootstrap = bb.mean(a_r.values, n_replications=boot_reps)\n",
    "        b_bootstrap = bb.mean(b_r.values, n_replications=boot_reps)\n",
    "        \n",
    "        return a_bootstrap, b_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:54.486275Z",
     "start_time": "2019-02-20T21:40:52.985010Z"
    }
   },
   "outputs": [],
   "source": [
    "a_bootstrap, b_bootstrap = bayesian_bootstrap_analysis(df, col_name='Ratio_Nav_Search_to_Rel', boot_reps=4000, seed = 1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:54.500358Z",
     "start_time": "2019-02-20T21:40:54.488755Z"
    }
   },
   "outputs": [],
   "source": [
    "def bb_hdi(a_bootstrap, b_bootstrap, alpha = 0.05):\n",
    "    \"\"\"Calculate a 1-alpha high density interval\n",
    "    \n",
    "    Args:\n",
    "        a_bootstrap: a list of resampled means from page A journeys.\n",
    "        b_bootstrap: a list of resampled means from page B journeys.\n",
    "\n",
    "    Returns:\n",
    "        a_ci_low: the lower point of the 1-alpha% highest density interval for A.\n",
    "        a_ci_hi: the higher point of the 1-alpha% highest density interval for A.\n",
    "        b_ci_low: the lower point of the 1-alpha% highest density interval for B.\n",
    "        b_ci_hi: the higher point of the 1-alpha% highest density interval for B.\n",
    "        ypa_diff_mean: the mean difference for the posterior between A's and B's distributions.\n",
    "        ypa_diff_ci_low: lower hdi for posterior of the difference.\n",
    "        ypa_diff_ci_hi: upper hdi for posterior of the difference.\n",
    "        sorta_p_value: number of values greater than 0 divided by num of obs for mean diff psoterior.\n",
    "        \"\"\"\n",
    "    # Calculate a 95% HDI\n",
    "    a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap, alpha=alpha)\n",
    "    # Calculate a 95% HDI\n",
    "    b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap, alpha=alpha)\n",
    "    \n",
    "    # calculate the posterior for the difference between A's and B's mean of resampled means\n",
    "    # ypa prefix is vestigial from blog post\n",
    "    ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "    ypa_diff_mean = ypa_diff.mean()\n",
    "    # get the hdi\n",
    "    ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "    # We count the number of values greater than 0 and divide by the total number\n",
    "    # of observations\n",
    "    # which returns us the the proportion of values in the distribution that are\n",
    "    # greater than 0, could act a bit like a p-value\n",
    "    p_value = (ypa_diff > 0).sum() / ypa_diff.shape[0]\n",
    "\n",
    "    return {'a_ci_low':a_ci_low, 'a_ci_hi':a_ci_hi, 'b_ci_low':b_ci_low, 'b_ci_hi':b_ci_hi, 'ypa_diff_mean':ypa_diff_mean, 'ypa_diff_ci_low':ypa_diff_ci_low, 'ypa_diff_ci_hi':ypa_diff_ci_hi, 'p_value':p_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:40:54.511808Z",
     "start_time": "2019-02-20T21:40:54.502568Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_stats = bb_hdi(a_bootstrap, b_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T19:50:33.315117Z",
     "start_time": "2019-02-20T19:50:33.258291Z"
    }
   },
   "outputs": [],
   "source": [
    "# This can be archived after new function above is tested and reviewed.\n",
    "\n",
    "# This original logic was converted into the above functions. Functions might be faster as they don't \n",
    "# chain the pandas dataframes, should test with 10000 resamplings on full dataset.\n",
    "# For tutorial see...\n",
    "# http://savvastjortjoglou.com/nfl-bayesian-bootstrap.html\n",
    "\n",
    "# need to roll out the data, to resample from, deaggregate on one variable of interest\n",
    "# we want to repeat each row's journey length by it's occurrences\n",
    "# so more common journey lengths are more likely to be sampled\n",
    "\n",
    "# r for ratio\n",
    "# a_r = np.repeat(df[df.ABVariant == \"A\"].Ratio_Nav_Search_to_Rel, df[df.ABVariant == \"A\"].Occurrences)\n",
    "# b_r = np.repeat(df[df.ABVariant == \"B\"].Ratio_Nav_Search_to_Rel, df[df.ABVariant == \"B\"].Occurrences)\n",
    "\n",
    "# for reproducibility, set the seed within this context\n",
    "# with NumpyRNGContext(1337):\n",
    "#    a_bootstrap = bb.mean(a_r.values, n_replications=boot_reps)\n",
    "#    b_bootstrap = bb.mean(b_r.values, n_replications=boot_reps)\n",
    "    \n",
    "# Calculate a 95% HDI\n",
    "# a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap)\n",
    "# print('low ci:', a_ci_low, '\\nhigh ci:', a_ci_hi)\n",
    "\n",
    "# Calculate a 95% HDI\n",
    "# b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap)\n",
    "# print('low ci:', b_ci_low, '\\nhigh ci:', b_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:43:58.924269Z",
     "start_time": "2019-02-20T21:43:58.919980Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_stats['b_ci_low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:49:06.997523Z",
     "start_time": "2019-02-20T21:49:06.543874Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax.errorbar(x=[ratio_stats['b_ci_low'], ratio_stats['b_ci_hi']], y=[2, 2], linewidth=5, c='teal', marker='o', \n",
    "         label='95% HDI B')\n",
    "\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.errorbar(x=[ratio_stats['a_ci_low'], ratio_stats['a_ci_hi']], y=[5, 5], linewidth=5, c='salmon', marker='o', \n",
    "         label='95% HDI A')\n",
    "\n",
    "ax.set(xlabel='Ratio of clicks on nav to clicks on related links', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:50:02.670521Z",
     "start_time": "2019-02-20T21:50:02.661783Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's ratio\n",
    "# ypa prefix is vestigial from blog post\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "\n",
    "# the mean of the posterior\n",
    "print('mean:', ypa_diff.mean())\n",
    "\n",
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:50:03.808825Z",
     "start_time": "2019-02-20T21:50:03.483980Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Ratio of nav events to related link events', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean ratio')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:50:08.989753Z",
     "start_time": "2019-02-20T21:50:08.984988Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proportion of journeys with a page sequence including content and related links only\n",
    "\n",
    "There is no statistically significant difference in the proportion of journeys with a page sequence including content and related links only (including loops) between page variant A and page variant B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys that only contain content pages and related links (i.e. no nav pages)}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:43:11.574281Z",
     "start_time": "2019-02-18T14:43:11.563606Z"
    }
   },
   "outputs": [],
   "source": [
    "# overall\n",
    "df[df.Content_Page_Nav_Event_Count == 0].Occurrences.sum() / df.Occurrences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T17:13:51.806312Z",
     "start_time": "2019-02-13T17:13:51.803513Z"
    }
   },
   "source": [
    "### Frequentist statistics\n",
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:45:09.093531Z",
     "start_time": "2019-02-18T14:45:09.075707Z"
    }
   },
   "outputs": [],
   "source": [
    "nav = z_prop(df, 'Content_Page_Nav_Event_Count')\n",
    "nav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:45:11.413865Z",
     "start_time": "2019-02-18T14:45:11.408164Z"
    }
   },
   "outputs": [],
   "source": [
    "# function defined earlier in notebook\n",
    "\n",
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(nav['x_a'], nav['n_a'],\n",
    "                                           nav['x_b'], nav['n_b'], alpha = 0.01)\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Journey Length (number of page views)\n",
    "There is no statistically significant difference in the average page list length of journeys (including loops) between page variant A and page variant B.\n",
    "### Bayesian bootstrap for non-parametric hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T10:51:26.103446Z",
     "start_time": "2019-02-15T10:51:26.100848Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://savvastjortjoglou.com/nfl-bayesian-bootstrap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T10:51:26.108656Z",
     "start_time": "2019-02-15T10:51:26.105421Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's use mean journey length (could probably model parametrically but we use it for demonstration here)\n",
    "# some journeys have length 500 and should probably be removed as they are liekely bots or other weirdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:52.195077Z",
     "start_time": "2019-02-20T21:53:52.185350Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to roll out the data, deaggregate on one variable of interest\n",
    "# we want to repeat each row's journey length by it's occurrences\n",
    "# so more common journey lengths are more likely to be sampled\n",
    "print(df['Page_List_Length'].head())\n",
    "print(df['Occurrences'].head())\n",
    "\n",
    "np.repeat(df['Page_List_Length'].head(), df['Occurrences'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:53.347148Z",
     "start_time": "2019-02-20T21:53:53.333740Z"
    }
   },
   "outputs": [],
   "source": [
    "a_len = np.repeat(df.loc[df.ABVariant == \"A\", 'Page_List_Length'], df.loc[df.ABVariant == \"A\", \"Occurrences\"])\n",
    "a_len.values\n",
    "\n",
    "b_len = np.repeat(df.loc[df.ABVariant == \"B\", 'Page_List_Length'], df.loc[df.ABVariant == \"B\", \"Occurrences\"])\n",
    "b_len.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:54.237213Z",
     "start_time": "2019-02-20T21:53:54.232994Z"
    }
   },
   "outputs": [],
   "source": [
    "help(bb.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:56.556684Z",
     "start_time": "2019-02-20T21:53:54.988122Z"
    }
   },
   "outputs": [],
   "source": [
    "# for reproducibility, set the seed within this context\n",
    "with NumpyRNGContext(1337):\n",
    "    a_bootstrap = bb.mean(a_len.values, n_replications=boot_reps)\n",
    "    b_bootstrap = bb.mean(b_len.values, n_replications=boot_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:56.871793Z",
     "start_time": "2019-02-20T21:53:56.559351Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(a_bootstrap, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant A Mean Journey Length')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:56.880254Z",
     "start_time": "2019-02-20T21:53:56.874255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap)\n",
    "print('low ci:', a_ci_low, '\\nhigh ci:', a_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:58.034765Z",
     "start_time": "2019-02-20T21:53:57.696106Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(a_bootstrap, color='salmon')\n",
    "ax.plot([a_ci_low, a_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant A Mean Journey Length')\n",
    "sns.despine()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:58.581163Z",
     "start_time": "2019-02-20T21:53:58.574391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap)\n",
    "print('low ci:', b_ci_low, '\\nhigh ci:', b_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:53:59.919129Z",
     "start_time": "2019-02-20T21:53:59.596151Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap)\n",
    "ax.plot([b_ci_low, b_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant B Mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:01.541873Z",
     "start_time": "2019-02-20T21:54:00.992916Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the uncertainty in the difference between the Page Variants's Journey Length by subtracting their posteriors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:04.518309Z",
     "start_time": "2019-02-20T21:54:04.512280Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's YPA\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:05.226192Z",
     "start_time": "2019-02-20T21:54:05.221955Z"
    }
   },
   "outputs": [],
   "source": [
    "# the mean of the posterior\n",
    "ypa_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:06.183907Z",
     "start_time": "2019-02-20T21:54:06.179628Z"
    }
   },
   "outputs": [],
   "source": [
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:07.505714Z",
     "start_time": "2019-02-20T21:54:07.191188Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually calculate the probability that B's mean Journey Length was greater than A's mean Journey Length by measuring the proportion of values greater than 0 in the above distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:54:10.428011Z",
     "start_time": "2019-02-20T21:54:10.423050Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.15px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
