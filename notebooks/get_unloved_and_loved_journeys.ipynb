{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:36:59.291030Z",
     "start_time": "2019-03-05T13:36:57.034500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "from datetime import datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data (content items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:37:00.589091Z",
     "start_time": "2019-03-05T13:36:59.294452Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata/clean_content_links.csv')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only pages with related links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:30.577892Z",
     "start_time": "2019-03-05T13:38:30.489351Z"
    }
   },
   "outputs": [],
   "source": [
    "# only select rows that have related links (they have \n",
    "# related_mainstream_content, ordered_related_items, or quick_links)\n",
    "clean_content_rl_df = clean_content_df.copy().query(\n",
    "    'related_mainstream_content.notnull() or ordered_related_items.notnull()or part_of_step_navs.notnull() or quick_links.notnull()'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy related link pages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:31.515900Z",
     "start_time": "2019-03-05T13:38:31.464231Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill NaNs with empty arrays, and then literal_eval all the arrays so we can\n",
    "# access the items within them (the different slugs associated with each\n",
    "# content ID)\n",
    "clean_content_rl_df['slugs'] = clean_content_rl_df['slugs'].fillna(\"['']\").apply(\n",
    "    ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converts the string to a list and puts an empty list where there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:32.667663Z",
     "start_time": "2019-03-05T13:38:32.664524Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_rl_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:33.667398Z",
     "start_time": "2019-03-05T13:38:33.664200Z"
    }
   },
   "outputs": [],
   "source": [
    "# sometimes there isn't an empty slug in the list of slugs, but the page path\n",
    "# exists, so this is a little hack to includ the plain basePath\n",
    "def add_dummy_slug(slugs):\n",
    "    list1 = ['']\n",
    "    list1.extend(slugs)\n",
    "    return list(set(list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummmy_slug is the url without any slug which  is also a page so needs an empty slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:37.345026Z",
     "start_time": "2019-03-05T13:38:37.338481Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_rl_df['slugs'] = clean_content_rl_df['slugs'].apply(add_dummy_slug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wide to long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:39.194625Z",
     "start_time": "2019-03-05T13:38:39.189133Z"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "def splitDataFrameList(df,target_column):\n",
    "    '''\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split, in an array\n",
    "    returns: a dataframe with each entry for the target column separated,\n",
    "        with each element moved into a new row. The values in the other\n",
    "        columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column):\n",
    "        for s in row[target_column]:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows,axis=1,args = (new_rows,target_column))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:38:42.779207Z",
     "start_time": "2019-03-05T13:38:42.515622Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_exploded_df = splitDataFrameList(\n",
    "    clean_content_rl_df, 'slugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T16:13:13.897719Z",
     "start_time": "2019-03-04T16:13:13.894423Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_slash_between_basepath_slug(slug):\n",
    "    if slug == '':\n",
    "        return ''\n",
    "    else:\n",
    "        return '/' + slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T16:13:14.450255Z",
     "start_time": "2019-03-04T16:13:14.444566Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_exploded_df['slug'] = clean_content_exploded_df['slugs'].map(\n",
    "    add_slash_between_basepath_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T16:13:15.103071Z",
     "start_time": "2019-03-04T16:13:15.094796Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_exploded_df['pagePath'] = clean_content_exploded_df['base_path']  + clean_content_exploded_df['slug']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T16:48:37.100274Z",
     "start_time": "2019-03-04T16:48:36.969169Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_exploded_df.to_csv(os.path.join(DATA_DIR, 'metadata/loved_pages.csv.gz'),\n",
    "                        compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMART answers needed separately as each answer gets a new slug so the beginning of the url is matched to classify these as loved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:35:25.388279Z",
     "start_time": "2019-03-05T11:35:25.364954Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content_exploded_df[\n",
    "    clean_content_exploded_df['document_type'] == 'simple_smart_answer'].to_csv(os.path.join(DATA_DIR, 'metadata/loved_smart_answers.csv.gz'),\n",
    "                        compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T09:37:14.577565Z",
     "start_time": "2019-03-05T09:37:14.574794Z"
    }
   },
   "outputs": [],
   "source": [
    "REQUIRED_COLUMNS = [\"Occurrences\", \"ABVariant\", \"Page_Event_List\",\n",
    "                    \"Page_List\",  \"Event_cat_act_agg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T09:33:21.459888Z",
     "start_time": "2019-03-05T09:33:21.424965Z"
    }
   },
   "outputs": [],
   "source": [
    "loved_pages_df = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'metadata/loved_pages.csv.gz'),\n",
    "    usecols=['pagePath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T09:37:13.456100Z",
     "start_time": "2019-03-05T09:37:13.449377Z"
    }
   },
   "outputs": [],
   "source": [
    "# dedupe the pagePaths here just in case\n",
    "loved_page_paths = list(set(loved_pages_df['pagePath'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T09:38:25.757449Z",
     "start_time": "2019-03-05T09:38:25.754148Z"
    }
   },
   "outputs": [],
   "source": [
    "loved_page_paths_set = set(loved_page_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:37:51.500048Z",
     "start_time": "2019-03-05T11:37:51.467158Z"
    }
   },
   "outputs": [],
   "source": [
    "loved_smart_answers_df = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'metadata/loved_smart_answers.csv.gz'),\n",
    "    usecols=['pagePath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:41:14.696234Z",
     "start_time": "2019-03-05T11:41:14.675511Z"
    }
   },
   "outputs": [],
   "source": [
    "loved_smart_answers_df['pagePath'] = loved_smart_answers_df['pagePath'] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:41:15.454159Z",
     "start_time": "2019-03-05T11:41:15.450864Z"
    }
   },
   "outputs": [],
   "source": [
    "loved_smart_answers = list(set(\n",
    "    loved_smart_answers_df['pagePath'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the pages come from links in this page https://www.gov.uk/government/organisations/hm-revenue-customs/contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:09:53.997886Z",
     "start_time": "2019-03-05T12:09:53.981379Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'metadata/hmrc_contact_pages.json'), \"r\") as read_file:\n",
    "    contact_pages = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:12:13.740610Z",
     "start_time": "2019-03-05T12:12:13.737380Z"
    }
   },
   "outputs": [],
   "source": [
    "hmrc_contact_pages = [link['base_path'] for link in contact_pages['links']['children']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:12:22.494437Z",
     "start_time": "2019-03-05T12:12:22.489137Z"
    }
   },
   "outputs": [],
   "source": [
    "hmrc_contact_pages_set = set(hmrc_contact_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:12:38.145108Z",
     "start_time": "2019-03-05T12:12:38.140612Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_loved_page(page):\n",
    "    return any([\n",
    "        re.match('/foreign-travel-advice/',page),\n",
    "        page in hmrc_contact_pages_set,\n",
    "        page in loved_page_paths_set,\n",
    "        page == '/help',\n",
    "        re.match('/premises-licence/',page),\n",
    "        page in ['/help/terms-conditions', \n",
    "         '/help/about-govuk',\n",
    "         '/help/accessibility', \n",
    "         '/help/privacy-policy',\n",
    "         '/help/cookies', \n",
    "         '/help/update-email-notifications',\n",
    "         '/help/browsers', \n",
    "         '/help/beta'],\n",
    "        re.match('/find-local-council/',page),\n",
    "        any([pagepath in page for pagepath in loved_smart_answers]), \n",
    "        ])\n",
    "\n",
    "# filter on Page_Event_List too in case it doesn't match Page_List - e.g. \n",
    "# when page hits happen before midnight but events happen after?\n",
    "def is_loved_page_event_list(page_event_list):\n",
    "    return any([is_loved_page(triple[0]) for triple in page_event_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:18:11.670357Z",
     "start_time": "2019-03-05T12:18:11.664297Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_daily_data(file_prefix):\n",
    "    print(f\"reading {file_prefix} data\")\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, f'processed_journey/taxon_ab_{file_prefix}.csv.gz'), \n",
    "        sep='\\t', \n",
    "        usecols=REQUIRED_COLUMNS)\n",
    "    print(\"page_list to literal list\")\n",
    "    df['Page_List'] = df['Page_List'].apply(ast.literal_eval)\n",
    "    print(\"Derive var: there_is_atleastone_loved_page\")\n",
    "    df['there_is_atleastone_loved_page'] = df.Page_List.map(lambda x: any([is_loved_page(page.split('?')[0]) for page in x]))\n",
    "    print(\"Derive var: there_is_atleastone_loved_event\")\n",
    "    df['there_is_atleastone_loved_event'] = is_loved_page_event_list(df['Page_Event_List'])\n",
    "    print(\"Derive var: is_loved_journey\")\n",
    "    df = df.assign(is_loved_journey = np.where((df.there_is_atleastone_loved_page==1) |\n",
    "                                                 (df.there_is_atleastone_loved_event==1), \n",
    "                                                 True, False))\n",
    "    \n",
    "    print(\"Number of occurences of journeys of this type\")\n",
    "    print(df[['Occurrences', 'is_loved_journey']].groupby('is_loved_journey').sum())\n",
    "    percent = df[['Occurrences', 'is_loved_journey']].groupby('is_loved_journey').sum().iloc[0]/(df[['Occurrences', \n",
    "                                                    'is_loved_journey']].groupby('is_loved_journey').sum().iloc[0] + \n",
    "                                                df[['Occurrences', 'is_loved_journey']].groupby('is_loved_journey').sum().iloc[1])\n",
    "    print(\"{:2.2%} of journeys are unloved on {}, a {}\".format(percent.item(), file_prefix, datetime.strptime(file_prefix, '%Y-%m-%d').strftime(\"%A\")))\n",
    "    print(\"writing files\")\n",
    "\n",
    "    df[df['is_loved_journey']==False].to_csv(os.path.join(DATA_DIR, f'processed_journey/unloved_{file_prefix}.csv.gz'), \n",
    "                                             sep=\"\\t\", \n",
    "                                             compression=\"gzip\", index=False)\n",
    "    df[df['is_loved_journey']==True].to_csv(\n",
    "        os.path.join(DATA_DIR, f'processed_journey/loved_{file_prefix}.csv.gz'), \n",
    "        sep=\"\\t\", \n",
    "        compression=\"gzip\", \n",
    "        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loved_and_unloved = sorted(glob.glob(\n",
    "        f'{DATA_DIR}/processed_journey/taxon_ab_*.csv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 2019-02-14 data\n",
      "page_list to literal list\n",
      "Derive var: there_is_atleastone_loved_page\n",
      "Derive var: there_is_atleastone_loved_event\n",
      "Derive var: is_loved_journey\n",
      "Number of occurences of journeys of this type\n",
      "                  Occurrences\n",
      "is_loved_journey             \n",
      "False                  904607\n",
      "True                  2607347\n",
      "25.76% of journeys are unloved on 2019-02-14, a Thursday\n",
      "writing files\n",
      "reading 2019-02-15 data\n",
      "page_list to literal list\n",
      "Derive var: there_is_atleastone_loved_page\n",
      "Derive var: there_is_atleastone_loved_event\n",
      "Derive var: is_loved_journey\n",
      "Number of occurences of journeys of this type\n",
      "                  Occurrences\n",
      "is_loved_journey             \n",
      "False                  832970\n",
      "True                  2486220\n",
      "25.10% of journeys are unloved on 2019-02-15, a Friday\n",
      "writing files\n",
      "reading 2019-02-16 data\n",
      "page_list to literal list\n",
      "Derive var: there_is_atleastone_loved_page\n",
      "Derive var: there_is_atleastone_loved_event\n",
      "Derive var: is_loved_journey\n",
      "Number of occurences of journeys of this type\n",
      "                  Occurrences\n",
      "is_loved_journey             \n",
      "False                  425419\n",
      "True                  1619383\n",
      "20.80% of journeys are unloved on 2019-02-16, a Saturday\n",
      "writing files\n",
      "reading 2019-02-17 data\n",
      "page_list to literal list\n",
      "Derive var: there_is_atleastone_loved_page\n",
      "Derive var: there_is_atleastone_loved_event\n",
      "Derive var: is_loved_journey\n",
      "Number of occurences of journeys of this type\n",
      "                  Occurrences\n",
      "is_loved_journey             \n",
      "False                  429250\n",
      "True                  1642816\n",
      "20.72% of journeys are unloved on 2019-02-17, a Sunday\n",
      "writing files\n",
      "reading 2019-02-18 data\n",
      "page_list to literal list\n",
      "Derive var: there_is_atleastone_loved_page\n",
      "Derive var: there_is_atleastone_loved_event\n",
      "Derive var: is_loved_journey\n",
      "Number of occurences of journeys of this type\n",
      "                  Occurrences\n",
      "is_loved_journey             \n",
      "False                 1034599\n",
      "True                  3098418\n",
      "25.03% of journeys are unloved on 2019-02-18, a Monday\n",
      "writing files\n"
     ]
    }
   ],
   "source": [
    "for file in combined_loved_and_unloved:\n",
    "    date = re.search(r'\\d{4}-\\d{2}-\\d{2}', file)\n",
    "    split_daily_data(date.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
