{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B test 1 - unloved journeys, control vs content similarity sorted list\n",
    "\n",
    "This related links A/B test (ab1) was conducted from 14-26th Feb 2019.\n",
    "The data used in this report are 14-18th Feb 2019.\n",
    "\n",
    "It compared the existing related links (wherhe available) to links generated using Google's universal sentence encoder V2. The first 300 words of content tagged to the topic taxonomy (labelled.csv.gz) were encoded and cosine distance was used to find the nearest vector to each content vector. A maximum of 5 links were suggested and only links above a threshold of 0.15 were suggested. \n",
    "\n",
    "16% of labelled content did not recieve any suggested links and 63% of content received 5 suggested links:\n",
    "\n",
    "| number of suggested links | % pages with this number of links |\n",
    "|--:|----:|\n",
    "| 0 | 16% |\n",
    "| 1 |  8% |\n",
    "| 2 |  5% |\n",
    "| 3 | 4%  |\n",
    "| 4 | 3%  |\n",
    "| 5 | 63% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:40:55.785127Z",
     "start_time": "2019-02-26T13:40:54.355834Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# z test\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# bayesian bootstrap and vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bayesian_bootstrap.bootstrap as bb\n",
    "from astropy.utils import NumpyRNGContext\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# set up the style for our plots\n",
    "sns.set(style='white', palette='colorblind', font_scale=1.3,\n",
    "        rc={'figure.figsize':(12,9), \n",
    "            \"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# instantiate progress bar goodness\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "# the number of bootstrap means used to generate a distribution\n",
    "boot_reps = 10000\n",
    "\n",
    "# alpha - false positive rate\n",
    "alpha = 0.05\n",
    "# number of tests\n",
    "m = 4\n",
    "# Correct alpha for multiple comparisons\n",
    "alpha = alpha / m\n",
    "\n",
    "# The Bonferroni correction can be used to adjust confidence intervals also. \n",
    "# If one establishes m confidence intervals, and wishes to have an overall confidence level of 1-alpha,\n",
    "# each individual confidence interval can be adjusted to the level of 1-(alpha/m).\n",
    "\n",
    "# reproducible\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File/dir locations\n",
    "### Processed journey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:40:55.798452Z",
     "start_time": "2019-02-26T13:40:55.790046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ellieking/Documents/govuk_ab_analysis/data/sampled_journey/full_sample_unloved_947858.csv.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "filename = \"full_sample_unloved_947858.csv.gz\"\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"sampled_journey\",\n",
    "    filename)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:40:57.151773Z",
     "start_time": "2019-02-26T13:40:55.801901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529111/529111 [00:24<00:00, 21590.29it/s]\n",
      "100%|██████████| 529111/529111 [00:35<00:00, 14782.47it/s]\n",
      "100%|██████████| 529111/529111 [00:09<00:00, 56872.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# read in processed sampled journey with just the cols we need for related links\n",
    "df = pd.read_csv(filepath, sep =\"\\t\", compression=\"gzip\")\n",
    "# convert from str to list\n",
    "df['Event_cat_act_agg']= df['Event_cat_act_agg'].progress_apply(ast.literal_eval)\n",
    "df['Page_Event_List'] = df['Page_Event_List'].progress_apply(ast.literal_eval)\n",
    "df['Page_List'] = df['Page_List'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:40:57.178028Z",
     "start_time": "2019-02-26T13:40:57.157068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529111/529111 [00:00<00:00, 761284.49it/s]\n"
     ]
    }
   ],
   "source": [
    "df['Page_List_Length'] = df['Page_List'].progress_apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:40:57.195591Z",
     "start_time": "2019-02-26T13:40:57.184003Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop dodgy rows, where page variant is not A or B. \n",
    "df = df.query('ABVariant in [\"A\", \"B\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nav type of page lookup - is it a finding page? if not it's a thing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.123584Z",
     "start_time": "2019-02-26T13:40:57.197189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ellieking/Documents/govuk_ab_analysis/data/metadata/document_types.csv.gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagePath</th>\n",
       "      <th>is_finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/jobsearch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pay-leave-for-parents/y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/student-finance-calculator/y/2018-2019/uk-full-time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/government/publications/application-for-confirmation-of-british-nationality-status-form-ns</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/pip/eligibility</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      pagePath  \\\n",
       "0                                                                                   /jobsearch   \n",
       "1                                                                     /pay-leave-for-parents/y   \n",
       "2                                         /student-finance-calculator/y/2018-2019/uk-full-time   \n",
       "3  /government/publications/application-for-confirmation-of-british-nationality-status-form-ns   \n",
       "4                                                                             /pip/eligibility   \n",
       "\n",
       "   is_finding  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"document_types.csv.gz\"\n",
    "\n",
    "# created a metadata dir in the DATA_DIR to hold this data\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"metadata\",\n",
    "    filename)\n",
    "print(filepath)\n",
    "\n",
    "df_finding_thing = pd.read_csv(filepath, sep=\"\\t\", compression=\"gzip\")\n",
    "\n",
    "df_finding_thing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.352020Z",
     "start_time": "2019-02-26T13:41:01.125929Z"
    }
   },
   "outputs": [],
   "source": [
    "thing_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==0]['pagePath'].tolist()\n",
    "\n",
    "\n",
    "finding_page_paths = df_finding_thing[\n",
    "    df_finding_thing['is_finding']==1]['pagePath'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Some rows should be removed before analysis. For example rows with journey lengths of 500 or very high related link click rates. This process might have to happen once features have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## journey_click_rate\n",
    "There is no difference in the proportion of journeys using at least one related link (journey_click_rate) between page variant A and page variant B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys including at least one click on a related link}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related link prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.362548Z",
     "start_time": "2019-02-26T13:41:01.354218Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_events_rl(event):\n",
    "    \"\"\"Counts events with category 'relatedLinkClicked' and action'Related content'.\"\"\"\n",
    "    if event[0][0] == 'relatedLinkClicked' and 'Related content' in event[0][1]:\n",
    "        return event[1]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sum_related_click_events(event_list):\n",
    "    return sum([get_number_of_events_rl(event) for event in event_list])\n",
    "\n",
    "\n",
    "def is_related(x):\n",
    "    \"\"\"Compute whether a journey includes at least one related link click.\"\"\"\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.386488Z",
     "start_time": "2019-02-26T13:41:01.365933Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of related links clicks per Sequence\n",
    "df['Related Links Clicks per seq'] = df['Event_cat_act_agg'].map(sum_related_click_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.399978Z",
     "start_time": "2019-02-26T13:41:01.389676Z"
    }
   },
   "outputs": [],
   "source": [
    "# map across the Sequence variable, which includes pages and Events\n",
    "# we want to pass all the list elements to a function one-by-one and then collect the output.\n",
    "df[\"Has_Related\"] = df[\"Related Links Clicks per seq\"].map(is_related)\n",
    "\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.431258Z",
     "start_time": "2019-02-26T13:41:01.401830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABVariant</th>\n",
       "      <th>Page_Event_List</th>\n",
       "      <th>Page_List</th>\n",
       "      <th>Event_cat_act_agg</th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Page_List_Length</th>\n",
       "      <th>Related Links Clicks per seq</th>\n",
       "      <th>Has_Related</th>\n",
       "      <th>Related Links Clicks row total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>[(/ design address, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other)]</td>\n",
       "      <td>[/ design address, /]</td>\n",
       "      <td>[((PAGE_NULL, PAGE_NULL), 2)]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>[(/ equality and diversity, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/government/publications/equality-and-diversity-scheme, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, e491505c-77ae-45b2-84be-8c94b94f6a2b), (/government/publications/equality-and-diversity-scheme, EVENT&lt;:&lt;Download Link Clicked&lt;:&lt;/government/uploads/system/uploads/attachment_data/file/28142/Equality_Diversity_2008_2011.pdf, e491505c-77ae-45b2-84be-8c94b94f6a2b), (/government/publications/equality-and-diversity-scheme, EVENT&lt;:&lt;Download Link Clicked&lt;:&lt;/government...</td>\n",
       "      <td>[/ equality and diversity, /government/publications/equality-and-diversity-scheme]</td>\n",
       "      <td>[((PAGE_NULL, PAGE_NULL), 2), ((Download Link Clicked, /government/uploads/system/uploads/attachment_data/file/28142/Equality_Diversity_2008_2011.pdf), 2)]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>[(/ government/pubiications/ supplier-code-of-conduct, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/govenment/publications.supplier-code-of-conduct, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/government/publications.supplier-code-of-conduct, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/government/publications.supplier-code-of-conduct, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, other), (/government/publications/supplier-code-of-conduct, PAGE&lt;:&lt;NULL&lt;:&lt;NULL, f3caf326-fe33-410f-b7f4-553f4011c81e), (/government/publications/supplier-code-of-conduct, EVENT&lt;:&lt;Ext...</td>\n",
       "      <td>[/ government/pubiications/ supplier-code-of-conduct, /govenment/publications.supplier-code-of-conduct, /government/publications.supplier-code-of-conduct, /government/publications.supplier-code-of-conduct, /government/publications/supplier-code-of-conduct, /government/publications/supplier-code-of-conduct]</td>\n",
       "      <td>[((PAGE_NULL, PAGE_NULL), 6), ((External Link Clicked, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/646497/2017-09-13_Official_Sensitive_Supplier_Code_of_Conduct_September_2017.pdf), 1), ((Download Link Clicked, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/646497/2017-09-13_Official_Sensitive_Supplier_Code_of_Conduct_September_2017.pdf), 1), ((External Link Clicked, https://assets.publishing...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ABVariant  \\\n",
       "0         A   \n",
       "1         A   \n",
       "2         A   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Page_Event_List  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                      [(/ design address, PAGE<:<NULL<:<NULL, other), (/, PAGE<:<NULL<:<NULL, other)]   \n",
       "1  [(/ equality and diversity, PAGE<:<NULL<:<NULL, other), (/government/publications/equality-and-diversity-scheme, PAGE<:<NULL<:<NULL, e491505c-77ae-45b2-84be-8c94b94f6a2b), (/government/publications/equality-and-diversity-scheme, EVENT<:<Download Link Clicked<:</government/uploads/system/uploads/attachment_data/file/28142/Equality_Diversity_2008_2011.pdf, e491505c-77ae-45b2-84be-8c94b94f6a2b), (/government/publications/equality-and-diversity-scheme, EVENT<:<Download Link Clicked<:</government...   \n",
       "2  [(/ government/pubiications/ supplier-code-of-conduct, PAGE<:<NULL<:<NULL, other), (/govenment/publications.supplier-code-of-conduct, PAGE<:<NULL<:<NULL, other), (/government/publications.supplier-code-of-conduct, PAGE<:<NULL<:<NULL, other), (/government/publications.supplier-code-of-conduct, PAGE<:<NULL<:<NULL, other), (/government/publications/supplier-code-of-conduct, PAGE<:<NULL<:<NULL, f3caf326-fe33-410f-b7f4-553f4011c81e), (/government/publications/supplier-code-of-conduct, EVENT<:<Ext...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                             Page_List  \\\n",
       "0                                                                                                                                                                                                                                                                                                [/ design address, /]   \n",
       "1                                                                                                                                                                                                                                   [/ equality and diversity, /government/publications/equality-and-diversity-scheme]   \n",
       "2  [/ government/pubiications/ supplier-code-of-conduct, /govenment/publications.supplier-code-of-conduct, /government/publications.supplier-code-of-conduct, /government/publications.supplier-code-of-conduct, /government/publications/supplier-code-of-conduct, /government/publications/supplier-code-of-conduct]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Event_cat_act_agg  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [((PAGE_NULL, PAGE_NULL), 2)]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                          [((PAGE_NULL, PAGE_NULL), 2), ((Download Link Clicked, /government/uploads/system/uploads/attachment_data/file/28142/Equality_Diversity_2008_2011.pdf), 2)]   \n",
       "2  [((PAGE_NULL, PAGE_NULL), 6), ((External Link Clicked, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/646497/2017-09-13_Official_Sensitive_Supplier_Code_of_Conduct_September_2017.pdf), 1), ((Download Link Clicked, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/646497/2017-09-13_Official_Sensitive_Supplier_Code_of_Conduct_September_2017.pdf), 1), ((External Link Clicked, https://assets.publishing...   \n",
       "\n",
       "   Occurrences  Page_List_Length  Related Links Clicks per seq  Has_Related  \\\n",
       "0            1                 2                             0        False   \n",
       "1            1                 2                             0        False   \n",
       "2            1                 6                             0        False   \n",
       "\n",
       "   Related Links Clicks row total  \n",
       "0                               0  \n",
       "1                               0  \n",
       "2                               0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.531050Z",
     "start_time": "2019-02-26T13:41:01.434069Z"
    }
   },
   "outputs": [],
   "source": [
    "def z_prop(df, col_name):\n",
    "    \"\"\"\n",
    "    Conduct z_prop test and generate confidence interval.\n",
    "\n",
    "    Using Bernoulli trial terminology where X (or x)\n",
    "    is number of successes and n is number of trials\n",
    "    total occurrences, we compare ABVariant A and B.\n",
    "    p is x/n. We use a z proportion test between variants.\n",
    "    \"\"\"\n",
    "    # A & B\n",
    "    n = df.Occurrences.sum()\n",
    "    # prop of journeys with at least one related link, occurrences summed for those rows gives X\n",
    "    p = df[df[col_name] == 1].Occurrences.sum() / n\n",
    "\n",
    "    assert (p >= 0), \"Prop less than zero!\"\n",
    "    assert (p <= 1), \"Prop greater than one!\"\n",
    "\n",
    "    # A\n",
    "    # number of trials for page A\n",
    "    n_a = df[df.ABVariant == \"A\"].Occurrences.sum()\n",
    "    # number of successes (occurrences), for page A and at least one related link clicked journeys\n",
    "    x_a = df[(df['ABVariant'] == 'A') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on A\n",
    "    p_a = x_a / n_a\n",
    "\n",
    "    # B\n",
    "    # number of trials for page B\n",
    "    n_b = df[df.ABVariant == \"B\"].Occurrences.sum()\n",
    "    # number of successes for page B, at least one related link clicked\n",
    "    x_b = df[(df['ABVariant'] == 'B') & (df[col_name] == 1)].Occurrences.sum()\n",
    "    # prop of journeys where one related link was clicked, on B\n",
    "    p_b = x_b / n_b\n",
    "\n",
    "    assert (n == n_a + n_b), \"Error in filtering by ABVariant!\"\n",
    "\n",
    "    # validate assumptions\n",
    "    # The formula of z-statistic is valid only when sample size (n) is large enough.\n",
    "    # nAp, nAq, nBp and nBq should be ≥ 5.\n",
    "    # where p is probability of success (we can use current baseline)\n",
    "    # q = 1 - p\n",
    "\n",
    "    # tried a helper function here but it didn't work hence not DRY\n",
    "    assert (n_a * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_a * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    assert (n_b * p) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "    assert (n_b * (1 - p)) >= 5, \"Assumptions for z prop test invalid!\"\n",
    "\n",
    "    # using statsmodels\n",
    "    # successes\n",
    "    count = np.array([x_a, x_b])\n",
    "    # number of trials\n",
    "    nobs = np.array([n_a, n_b])\n",
    "    # z prop test\n",
    "    z, p_value = proportions_ztest(count, nobs, value=0, alternative='two-sided')\n",
    "    # print(' z-stat = {z} \\n p-value = {p_value}'.format(z=z,p_value=p_value))\n",
    "\n",
    "    statsdict = {'metric_name': col_name, 'stats_method': 'z_prop_test',\n",
    "                 'x_ab': x_a + x_b, 'n_ab': n, 'p': p,\n",
    "                 'x_a': x_a, 'n_a': n_a, 'p_a': p_a,\n",
    "                 'x_b': x_b, 'n_b': n_b, 'p_b': p_b,\n",
    "                 'test_statistic': z, 'p-value': p_value}\n",
    "\n",
    "    return statsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.536503Z",
     "start_time": "2019-02-26T13:41:01.533218Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(proportions_ztest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.563539Z",
     "start_time": "2019-02-26T13:41:01.539456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric_name': 'Has_Related',\n",
       " 'stats_method': 'z_prop_test',\n",
       " 'x_ab': 11839,\n",
       " 'n_ab': 1895717,\n",
       " 'p': 0.006245130470423591,\n",
       " 'x_a': 21,\n",
       " 'n_a': 947858,\n",
       " 'p_a': 2.215521734268213e-05,\n",
       " 'x_b': 11818,\n",
       " 'n_b': 947859,\n",
       " 'p_b': 0.01246809915820813,\n",
       " 'test_statistic': -108.76125351232109,\n",
       " 'p-value': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_rel = z_prop(df, 'Has_Related')\n",
    "has_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.570081Z",
     "start_time": "2019-02-26T13:41:01.565956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_rel['p-value'] < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-tailed test for unloved content only. \n",
    "Is the proportion of related links greater than zero? The A variant for unloved content shouldn't have any related links to click on so should be zero. Those that are in there are due to misclassification which will be explored in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis: The proportion of journeys using related links in the B variant sample was drawn from a distribution with proportion equal to 0\n",
    "\n",
    "Alternative hypothesis: Proportion of journeys using related links > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000\n"
     ]
    }
   ],
   "source": [
    "n_b = df[df.ABVariant == \"B\"].Occurrences.sum()\n",
    "    # number of successes for page B, at least one related link clicked\n",
    "x_b = df[(df['ABVariant'] == 'B') & (df['Has_Related'] == 1)].Occurrences.sum()\n",
    "    \n",
    "stat, pval = proportions_ztest(x_b, n_b, 0)\n",
    "# if p < alpha then reject the data does not provde evidence for the null hypothesis\n",
    "print('{0:0.3f}'.format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.582089Z",
     "start_time": "2019-02-26T13:41:01.572976Z"
    }
   },
   "outputs": [],
   "source": [
    "# uplift\n",
    "def compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b):\n",
    "    \"\"\"\n",
    "    The standard error of the difference between two proportions is given by the square root of the variances.\n",
    "    \n",
    "    The square of the standard error of a proportion is known as the variance of proportion. \n",
    "    The variance of the difference between two independent proportions is equal to the sum of the variances of the proportions of each sample. \n",
    "    The variances are summed because each sample contributes to sampling error in the distribution of differences.\n",
    "    \n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = p1*(1-p1)/n_a + p2*(1-p2)/n_b\n",
    "    return np.sqrt(se)\n",
    "    \n",
    "def zconf_interval_two_samples(x_a, n_a, x_b, n_b, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Gives two points, the lower and upper bound of a (1-alpha)% confidence interval.\n",
    "    \n",
    "    To calculate the confidence interval we need to know the standard error of the difference between two proportions. \n",
    "    The standard error of the difference between two proportions is the combination of the standard error of two independent distributions, ES (p_a) and (p_b).\n",
    "    \n",
    "    If the CI includes zero then we accept the null hypothesis at the defined alpha.\n",
    "    \"\"\"\n",
    "    p1 = x_a/n_a\n",
    "    p2 = x_b/n_b    \n",
    "    se = compute_standard_error_prop_two_samples(x_a, n_a, x_b, n_b)\n",
    "    z_critical = stats.norm.ppf(1-0.5*alpha)\n",
    "    return p2-p1-z_critical*se, p2-p1+z_critical*se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.588740Z",
     "start_time": "2019-02-26T13:41:01.584173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " difference in proportions = 1.24%\n",
      " 95% Confidence Interval = ( 1.22% , 1.27% )\n"
     ]
    }
   ],
   "source": [
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(has_rel['x_a'], has_rel['n_a'],\n",
    "                                           has_rel['x_b'], has_rel['n_b'], alpha = alpha)\n",
    "print(' difference in proportions = {0:.2f}%'.format(100*(has_rel['x_b']/has_rel['n_b']-has_rel['x_a']/has_rel['n_a'])))\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.600657Z",
     "start_time": "2019-02-26T13:41:01.597659Z"
    }
   },
   "source": [
    "Based on [this](https://medium.com/@thibalbo/coding-bayesian-ab-tests-in-python-e89356b3f4bd) blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be developed, a Bayesian approach can provide a simpler interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count of clicks on navigation elements\n",
    "\n",
    "There is no statistically significant difference in the count of clicks on navigation elements per journey between page variant A and page variant B.\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\text{total number of navigation element click events from content pages}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:12:55.711730Z",
     "start_time": "2019-02-14T22:12:55.709272Z"
    }
   },
   "source": [
    "### Related link counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.610541Z",
     "start_time": "2019-02-26T13:41:01.605316Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the total number of related links clicks for that row (clicks per sequence multiplied by occurrences)\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:01.621493Z",
     "start_time": "2019-02-26T13:41:01.613851Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_nav_event(event):\n",
    "    \"\"\"\n",
    "    Return the total number of related links clicks for that row.\n",
    "    \n",
    "    Clicks per sequence multiplied by occurrences. \n",
    "    \"\"\"\n",
    "    return any(\n",
    "        ['breadcrumbClicked' in event, 'homeLinkClicked' in event,\n",
    "         all(cond in event for cond in [\n",
    "             'relatedLinkClicked','Explore the topic'])])\n",
    "\n",
    "\n",
    "def count_nav_events(page_event_list):\n",
    "    \"\"\"Counts the number of nav events from a content page in a Page Event List.\"\"\"\n",
    "    content_page_nav_events = 0\n",
    "    for pair in page_event_list:\n",
    "        if is_nav_event(pair[1]):\n",
    "            if pair[0] in thing_page_paths:\n",
    "                content_page_nav_events += 1\n",
    "    return content_page_nav_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:08.126938Z",
     "start_time": "2019-02-26T13:41:01.624130Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529111/529111 [10:04<00:00, 875.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# needs finding_thing_df read in from document_types.csv.gz\n",
    "df['Content_Page_Nav_Event_Count'] = df['Page_Event_List'].progress_map(count_nav_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:08.134405Z",
     "start_time": "2019-02-26T13:41:08.129287Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_search_from_content(page_list):\n",
    "    search_from_content = 0\n",
    "    for i, page in enumerate(page_list):\n",
    "        if i > 0:\n",
    "            if '/search?q=' in page:\n",
    "                if page_list[i-1] in thing_page_paths:\n",
    "                    search_from_content += 1\n",
    "    return search_from_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.694350Z",
     "start_time": "2019-02-26T13:41:08.137854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 307053/529111 [26:11<12:09, 304.25it/s]"
     ]
    }
   ],
   "source": [
    "df['Content_Search_Event_Count'] = df['Page_List'].progress_map(count_search_from_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the derived metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.705497Z",
     "start_time": "2019-02-26T13:41:19.696940Z"
    }
   },
   "outputs": [],
   "source": [
    "# count of nav or search clicks\n",
    "df['Content_Nav_or_Search_Count'] = df['Content_Page_Nav_Event_Count'] + df['Content_Search_Event_Count']\n",
    "# occurrences is accounted for by the group by bit in our bayesian boot analysis function\n",
    "df['Content_Nav_Search_Event_Sum_row_total'] = df['Content_Nav_or_Search_Count'] * df['Occurrences']\n",
    "# required for journeys with no nav later\n",
    "df['Has_No_Nav_Or_Search'] = df['Content_Nav_Search_Event_Sum_row_total'] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vestigial metric which we dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.711371Z",
     "start_time": "2019-02-26T13:41:19.707988Z"
    }
   },
   "outputs": [],
   "source": [
    "# (nav events + search events) + 1 / related links clicked + 1\n",
    "# add one to numerator and denominator to avoid undesirable characteristics\n",
    "# not sure this has great utility as a proxy, seems volatile\n",
    "# df['Ratio_Nav_Search_to_Rel'] = (df['Content_Nav_Search_Event_Sum_row_total'] + 1) / (df['Related Links Clicks row total'] + 1)\n",
    "# sns.distplot(df['Ratio_Nav_Search_to_Rel'].values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This derived variable is problematic, should consider dropping it. Use counts of the numerator instead (as this could be modelled using generalised linear model), as related link clickedness is captured by the earlier metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary df file in case of crash\n",
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.716649Z",
     "start_time": "2019-02-26T13:41:19.713633Z"
    }
   },
   "outputs": [],
   "source": [
    "# create temp file incase we crash after all that hard work\n",
    "# uncomment the next cell if you want to save\n",
    "filepath = os.path.join(\n",
    "    DATA_DIR, \"rl_sampled_processed_journey\",\n",
    "    filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.720954Z",
     "start_time": "2019-02-26T13:41:19.718595Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv(filepath, sep=\"\\t\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.731026Z",
     "start_time": "2019-02-26T13:41:19.723746Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_bb(counter_X_keys, counter_X_vals, n_replications):\n",
    "    \"\"\"Simulate the posterior distribution of the mean.\n",
    "    Parameter X: The observed data (array like)\n",
    "    Parameter n_replications: The number of bootstrap replications to perform (positive integer)\n",
    "    Returns: Samples from the posterior\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    weights = np.random.dirichlet(counter_X_vals, n_replications)\n",
    "    for w in weights:\n",
    "        samples.append(np.dot(counter_X_keys, w))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:19.740534Z",
     "start_time": "2019-02-26T13:41:19.733057Z"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bootstrap_analysis(df, col_name=None, boot_reps = 10000, seed = 1337):\n",
    "    \"\"\"Run bayesian bootstrap on the mean of a variable of interest between Page Variants.\n",
    "    \n",
    "    Args:\n",
    "        df: A rl_sampled_processed pandas Datframe.\n",
    "        col_name: A string of the column of interest.\n",
    "\n",
    "    Returns:\n",
    "        a_bootstrap: a vector of boot_reps n resampled means from A.\n",
    "        b_bootstrap: a vector of boot_reps n resampled means from B.\n",
    "        \"\"\"\n",
    "    # grouped by length is vestigial, before this was generalised into a function\n",
    "    with NumpyRNGContext(seed):\n",
    "        A_grouped_by_length =  df[df.ABVariant == \"A\"].groupby(\n",
    "            col_name).sum().reset_index()\n",
    "        B_grouped_by_length =  df[df.ABVariant == \"B\"].groupby(\n",
    "            col_name).sum().reset_index()\n",
    "        a_bootstrap = mean_bb(A_grouped_by_length[col_name], \n",
    "        A_grouped_by_length['Occurrences'], \n",
    "                                 boot_reps)\n",
    "        b_bootstrap = mean_bb(B_grouped_by_length[col_name], \n",
    "                                 B_grouped_by_length['Occurrences'], \n",
    "                                 boot_reps)\n",
    "    \n",
    "    return a_bootstrap, b_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_searches(df, group):\n",
    "    searches = df[df.ABVariant == group].groupby(\n",
    "            'Content_Nav_or_Search_Count').sum().iloc[:, 0].reset_index(0)\n",
    "    total_searches = searches['Content_Nav_or_Search_Count']*searches['Occurrences']\n",
    "    return sum(total_searches)\n",
    "\n",
    "def compare_total_searches(df):\n",
    "    print(\"total searches in A = {}\".format(count_total_searches(df, \"A\")))\n",
    "    print(\"total searches in B = {}\".format(count_total_searches(df, \"B\")))\n",
    "    percent_diff = abs(count_total_searches(df, \"B\") - count_total_searches(df, \"A\"))/(count_total_searches(df, \"A\")+count_total_searches(df, \"B\"))*100\n",
    "    print(\"B has {0} more navigation or searches than A a {1:.2f}% overall difference\".format(count_total_searches(df, \"B\") - count_total_searches(df, \"A\"), percent_diff))\n",
    "    print(\"The relative change was {0:.2f}% from A to B\".format((count_total_searches(df, \"B\") - count_total_searches(df, \"A\"))/count_total_searches(df, \"A\")*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_total_searches(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_df_B = df[df.ABVariant == \"B\"].groupby(\n",
    "            'Content_Nav_or_Search_Count').sum().iloc[:, 0]\n",
    "plot_df_A = df[df.ABVariant == \"A\"].groupby(\n",
    "            'Content_Nav_or_Search_Count').sum().iloc[:, 0]\n",
    "\n",
    "ax.set_yscale('log')\n",
    "width =0.4\n",
    "ax = plot_df_B.plot.bar(label='B', position=1, width=width)\n",
    "ax = plot_df_A.plot.bar(label='A', color='salmon', position=0, width=width)\n",
    "plt.title(\"Unloved journeys\")\n",
    "plt.ylabel(\"Log(number of journeys)\")\n",
    "plt.xlabel(\"Number of uses of search/nav elements in journey\")\n",
    "\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.savefig('nav_counts_unloved_bar.png', dpi = 900, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:20.982129Z",
     "start_time": "2019-02-26T13:41:19.743052Z"
    }
   },
   "outputs": [],
   "source": [
    "a_bootstrap, b_bootstrap = bayesian_bootstrap_analysis(df, col_name='Content_Nav_or_Search_Count', boot_reps=boot_reps, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a_bootstrap).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a_bootstrap).mean() - (0.05 * np.array(a_bootstrap).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(b_bootstrap).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - np.array(b_bootstrap).mean()/np.array(a_bootstrap).mean())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:20.998275Z",
     "start_time": "2019-02-26T13:41:20.984411Z"
    }
   },
   "outputs": [],
   "source": [
    "def bb_hdi(a_bootstrap, b_bootstrap, alpha = 0.05):\n",
    "    \"\"\"Calculate a 1-alpha high density interval\n",
    "    \n",
    "    Args:\n",
    "        a_bootstrap: a list of resampled means from page A journeys.\n",
    "        b_bootstrap: a list of resampled means from page B journeys.\n",
    "\n",
    "    Returns:\n",
    "        a_ci_low: the lower point of the 1-alpha% highest density interval for A.\n",
    "        a_ci_hi: the higher point of the 1-alpha% highest density interval for A.\n",
    "        b_ci_low: the lower point of the 1-alpha% highest density interval for B.\n",
    "        b_ci_hi: the higher point of the 1-alpha% highest density interval for B.\n",
    "        ypa_diff_mean: the mean difference for the posterior between A's and B's distributions.\n",
    "        ypa_diff_ci_low: lower hdi for posterior of the difference.\n",
    "        ypa_diff_ci_hi: upper hdi for posterior of the difference.\n",
    "        sorta_p_value: number of values greater than 0 divided by num of obs for mean diff psoterior.\n",
    "        \"\"\"\n",
    "    # Calculate a 95% HDI\n",
    "    a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap, alpha=alpha)\n",
    "    # Calculate a 95% HDI\n",
    "    b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap, alpha=alpha)\n",
    "    \n",
    "    # calculate the posterior for the difference between A's and B's mean of resampled means\n",
    "    # ypa prefix is vestigial from blog post\n",
    "    ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "    ypa_diff_mean = ypa_diff.mean()\n",
    "    # get the hdi\n",
    "    ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "    # We count the number of values greater than 0 and divide by the total number\n",
    "    # of observations\n",
    "    # which returns us the the proportion of values in the distribution that are\n",
    "    # greater than 0, could act a bit like a p-value\n",
    "    p_value = (ypa_diff > 0).sum() / ypa_diff.shape[0]\n",
    "\n",
    "    return {'a_ci_low':a_ci_low, 'a_ci_hi':a_ci_hi, 'b_ci_low':b_ci_low, 'b_ci_hi':b_ci_hi, 'ypa_diff_mean':ypa_diff_mean, 'ypa_diff_ci_low':ypa_diff_ci_low, 'ypa_diff_ci_hi':ypa_diff_ci_hi, 'p_value':p_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:21.023471Z",
     "start_time": "2019-02-26T13:41:21.000742Z"
    }
   },
   "outputs": [],
   "source": [
    "# ratio is vestigial but we keep it here for convenience\n",
    "# it's actually a count but considers occurrences\n",
    "ratio_stats = bb_hdi(a_bootstrap, b_bootstrap, alpha=alpha)\n",
    "ratio_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:21.733991Z",
     "start_time": "2019-02-26T13:41:21.026698Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax.errorbar(x=[ratio_stats['b_ci_low'], ratio_stats['b_ci_hi']], y=[2, 2], linewidth=5, c='teal', marker='o', \n",
    "         label='95% HDI B')\n",
    "\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.errorbar(x=[ratio_stats['a_ci_low'], ratio_stats['a_ci_hi']], y=[5, 5], linewidth=5, c='salmon', marker='o', \n",
    "         label='95% HDI A')\n",
    "\n",
    "ax.set(xlabel='mean search/nav count per journey', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True, bbox_to_anchor=(0.75, 1), loc='best')\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.title(\"Unloved journeys\")\n",
    "\n",
    "plt.savefig('nav_counts_unloved.png', dpi = 900, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:21.753708Z",
     "start_time": "2019-02-26T13:41:21.736779Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's ratio\n",
    "# ypa prefix is vestigial from blog post\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "\n",
    "# the mean of the posterior\n",
    "print('mean:', ypa_diff.mean())\n",
    "\n",
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.153046Z",
     "start_time": "2019-02-26T13:41:21.758538Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Content_Nav_or_Search_Count', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean counts times occurrences')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.162986Z",
     "start_time": "2019-02-26T13:41:22.155899Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.162986Z",
     "start_time": "2019-02-26T13:41:22.155899Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values less than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# less than 0, could act a bit like a p-value\n",
    "(ypa_diff < 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ypa_diff>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ypa_diff<0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proportion of journeys with a page sequence including content and related links only\n",
    "\n",
    "There is no statistically significant difference in the proportion of journeys with a page sequence including content and related links only (including loops) between page variant A and page variant B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys that only contain content pages and related links (i.e. no nav pages)}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.173656Z",
     "start_time": "2019-02-26T13:41:22.165820Z"
    }
   },
   "outputs": [],
   "source": [
    "# if (Content_Nav_Search_Event_Sum == 0) that's our success\n",
    "# Has_No_Nav_Or_Search == 1 is a success\n",
    "# the problem is symmetrical so doesn't matter too much\n",
    "sum(df.Has_No_Nav_Or_Search * df.Occurrences) / df.Occurrences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.480883Z",
     "start_time": "2019-02-26T13:41:22.176599Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.Content_Nav_or_Search_Count.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T17:13:51.806312Z",
     "start_time": "2019-02-13T17:13:51.803513Z"
    }
   },
   "source": [
    "### Frequentist statistics\n",
    "#### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.508617Z",
     "start_time": "2019-02-26T13:41:22.483507Z"
    }
   },
   "outputs": [],
   "source": [
    "nav = z_prop(df, 'Has_No_Nav_Or_Search')\n",
    "nav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical significance - uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.517619Z",
     "start_time": "2019-02-26T13:41:22.511717Z"
    }
   },
   "outputs": [],
   "source": [
    "# function defined earlier in notebook\n",
    "\n",
    "# Due to multiple testing we used the Bonferroni correction for alpha\n",
    "ci_low,ci_upp = zconf_interval_two_samples(nav['x_a'], nav['n_a'],\n",
    "                                           nav['x_b'], nav['n_b'], alpha = alpha)\n",
    "diff = 100*(nav['x_b']/nav['n_b']-nav['x_a']/nav['n_a'])\n",
    "print(' difference in proportions = {0:.2f}%'.format(diff))\n",
    "print(' 95% Confidence Interval = ( {0:.2f}% , {1:.2f}% )'\n",
    "      .format(100*ci_low, 100*ci_upp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There was a {0: .2f}% relative change in the proportion of journeys not using search/nav elements\".format(100 * ((nav['p_b']-nav['p_a'])/nav['p_a'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Journey Length (number of page views)\n",
    "There is no statistically significant difference in the average page list length of journeys (including loops) between page variant A and page variant B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_B = df[df.ABVariant == \"B\"].groupby(\n",
    "            'Page_List_Length').sum().iloc[:, 0]\n",
    "lengthB_2 = length_B.reindex(np.arange(1, 501, 1), fill_value=0)\n",
    "\n",
    "length_A = df[df.ABVariant == \"A\"].groupby(\n",
    "            'Page_List_Length').sum().iloc[:, 0]\n",
    "lengthA_2 = length_A.reindex(np.arange(1, 501, 1), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(100, 30))\n",
    "\n",
    "ax.set_yscale('log')\n",
    "width = 0.4\n",
    "ax = lengthB_2.plot.bar(label='B', position=1, width=width)\n",
    "ax = lengthA_2.plot.bar(label='A', color='salmon', position=0, width=width)\n",
    "plt.xlabel('length', fontsize=1)\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian bootstrap for non-parametric hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.524234Z",
     "start_time": "2019-02-26T13:41:22.520742Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://savvastjortjoglou.com/nfl-bayesian-bootstrap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:22.530308Z",
     "start_time": "2019-02-26T13:41:22.527365Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's use mean journey length (could probably model parametrically but we use it for demonstration here)\n",
    "# some journeys have length 500 and should probably be removed as they are liekely bots or other weirdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude journeys of longer than 500 as these could be automated traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[df['Page_List_Length'] < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean number of pages in an unloved journey is {0:.3f}\".format(sum(df.Page_List_Length*df.Occurrences)/df.Occurrences.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:23.909953Z",
     "start_time": "2019-02-26T13:41:22.533208Z"
    }
   },
   "outputs": [],
   "source": [
    "# for reproducibility, set the seed within this context\n",
    "a_bootstrap, b_bootstrap = bayesian_bootstrap_analysis(df, col_name='Page_List_Length', boot_reps=boot_reps, seed = seed)\n",
    "a_bootstrap_short, b_bootstrap_short = bayesian_bootstrap_analysis(df_short, col_name='Page_List_Length', boot_reps=boot_reps, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a_bootstrap).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(b_bootstrap).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There's a relative change of {0:.2f}% from A to B\".format((np.array(b_bootstrap).mean()-np.array(a_bootstrap).mean())/np.array(a_bootstrap).mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(a_bootstrap_short).mean())\n",
    "print(np.array(b_bootstrap_short).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:23.919944Z",
     "start_time": "2019-02-26T13:41:23.912025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "a_ci_low, a_ci_hi = bb.highest_density_interval(a_bootstrap)\n",
    "print('low ci:', a_ci_low, '\\nhigh ci:', a_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:24.317434Z",
     "start_time": "2019-02-26T13:41:23.923114Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(a_bootstrap, color='salmon')\n",
    "ax.plot([a_ci_low, a_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant A Mean Journey Length')\n",
    "sns.despine()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:24.330427Z",
     "start_time": "2019-02-26T13:41:24.319904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "b_ci_low, b_ci_hi = bb.highest_density_interval(b_bootstrap)\n",
    "print('low ci:', b_ci_low, '\\nhigh ci:', b_ci_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:24.701345Z",
     "start_time": "2019-02-26T13:41:24.333455Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap)\n",
    "ax.plot([b_ci_low, b_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', title='Page Variant B Mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.204000Z",
     "start_time": "2019-02-26T13:41:24.704734Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap, label='B')\n",
    "ax = sns.distplot(a_bootstrap, label='A', ax=ax, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.title(\"Unloved journeys\")\n",
    "\n",
    "plt.savefig('journey_length_unloved.png', dpi = 900, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.204000Z",
     "start_time": "2019-02-26T13:41:24.704734Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(b_bootstrap_short, label='B')\n",
    "ax = sns.distplot(a_bootstrap_short, label='A', ax=ax, color='salmon')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the uncertainty in the difference between the Page Variants's Journey Length by subtracting their posteriors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.219953Z",
     "start_time": "2019-02-26T13:41:25.206556Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the posterior for the difference between A's and B's YPA\n",
    "ypa_diff = np.array(b_bootstrap) - np.array(a_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.229381Z",
     "start_time": "2019-02-26T13:41:25.222921Z"
    }
   },
   "outputs": [],
   "source": [
    "# the mean of the posterior\n",
    "ypa_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.236620Z",
     "start_time": "2019-02-26T13:41:25.232640Z"
    }
   },
   "outputs": [],
   "source": [
    "print('low ci:', ypa_diff_ci_low, '\\nhigh ci:', ypa_diff_ci_hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.606060Z",
     "start_time": "2019-02-26T13:41:25.239935Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='Journey Length', ylabel='Density', \n",
    "       title='The difference between B\\'s and A\\'s mean Journey Length')\n",
    "sns.despine()\n",
    "legend = plt.legend(frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually calculate the probability that B's mean Journey Length was greater than A's mean Journey Length by measuring the proportion of values greater than 0 in the above distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.616307Z",
     "start_time": "2019-02-26T13:41:25.609142Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:41:25.616307Z",
     "start_time": "2019-02-26T13:41:25.609142Z"
    }
   },
   "outputs": [],
   "source": [
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0, could act a bit like a p-value\n",
    "(ypa_diff < 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.15px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
